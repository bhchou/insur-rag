import pdfplumber
import json
import sys
import pytesseract
import requests
import re
from PIL import Image
import os

# ==========================================
# ğŸ”§ è¨­å®šå€ (è«‹ä¾æ“šæ‚¨çš„å¯¦éš›ç’°å¢ƒä¿®æ”¹)
# ==========================================
#
VLLM_ENDPOINT = os.getenv("VLLM_ENDPOINT")  # ä¾‹å¦‚: http://192.168.1.100:8000
MODEL_NAME = os.getenv("MODEL_NAME")              # ä¾‹å¦‚: meta-llama/Llama-3-8b-instruct
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

def clean_json_string(text):
    """
    æ¸…ç† LLM å›å‚³çš„å­—ä¸²ï¼Œç§»é™¤ Markdown æ¨™è¨˜ (```json ... ```)
    """
    # ç§»é™¤ ```json æˆ– ```
    cleaned = re.sub(r"```json\s*", "", text)
    cleaned = re.sub(r"```\s*", "", cleaned)
    return cleaned.strip()

def extract_metadata_via_llmYY(text_content):
    # 1. è®€å–è¨­å®š (ç¢ºä¿é€™è£¡è®€å¾—åˆ°æœ€æ–°çš„ .env)
    vllm_endpoint = os.getenv('VLLM_ENDPOINT', 'http://localhost:11434')
    bearer_token = os.getenv('BEARER_TOKEN')
    model_name = os.getenv('MODEL_NAME', 'qwen2.5:7b')

    # 2. è™•ç†ç¶²å€ (è·Ÿä¹‹å‰ä¸€æ¨£çš„ä¿®å¾©é‚è¼¯)
    base_url = vllm_endpoint.rstrip('/')
    if '/v1' in base_url:
        api_url = f"{base_url}/chat/completions"
    else:
        api_url = f"{base_url}/v1/chat/completions"

    # 3. è¨­å®š Header (è§£æ±º 401 çš„é—œéµ)
    headers = {
        "Content-Type": "application/json",
        "User-Agent": "PDF-Parser"
    }
    # åªæœ‰ç•¶ Token å­˜åœ¨ä¸”ä¸æ˜¯ 'none' æ™‚æ‰åŠ  Authorization
    if bearer_token and str(bearer_token).lower() not in ['none', '', 'null']:
        headers["Authorization"] = f"Bearer {bearer_token}"
        # print(f"DEBUG: Using Token {bearer_token[:3]}...") # é™¤éŒ¯ç”¨

    # 4. å®šç¾© Prompt (é€™è£¡ç¤ºç¯„å°‡ System Prompt èå…¥ User Prompt)
    # è«‹å°‡æ‚¨åŸæœ¬å¯«åœ¨ System Role çš„æŒ‡ä»¤è²¼åœ¨ system_instruction è®Šæ•¸è£¡
    system_instruction = """
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æª”åˆ†æåŠ©æ‰‹ã€‚
    è«‹åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œä¸¦æå–é—œéµ Metadata (å¦‚: æ—¥æœŸã€ä¿éšªå…¬å¸ã€éšªç¨®åç¨±)ã€‚
    è«‹ç›´æ¥è¼¸å‡º JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜ã€‚
    """
    
    final_prompt = f"{system_instruction}\n\n=== å¾…åˆ†ææ–‡æœ¬ ===\n{text_content}"

    # 5. å»ºæ§‹ Payload (å¼·åˆ¶ä½¿ç”¨å–®ä¸€ User Roleï¼Œç›¸å®¹æ€§æœ€é«˜)
    payload = {
        "model": model_name,
        "messages": [
            {
                "role": "user", 
                "content": final_prompt
            }
        ],
        "temperature": 0.1,
        "stream": False
    }

    try:
        # print(f"DEBUG: Posting to {api_url}") # é™¤éŒ¯ç”¨
        response = requests.post(api_url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result:
                content = result['choices'][0]['message']['content']
                # é€™è£¡å¯ä»¥åŠ ä¸€äº›ç°¡å–®çš„ JSON æ¸…ç†é‚è¼¯ (å»æ‰ ```json ...)
                clean_content = content.replace('```json', '').replace('```', '').strip()
                return clean_content
            
        elif response.status_code == 401:
            print(f"âŒ [PDF_PARSER] 401 æ¬Šé™éŒ¯èª¤! è«‹æª¢æŸ¥ .env çš„ BEARER_TOKEN")
            print(f"   é€£ç·šç›®æ¨™: {api_url}")
            return None
            
        else:
            print(f"âŒ [PDF_PARSER] API Error {response.status_code}: {response.text}")
            return None

    except Exception as e:
        print(f"âŒ [PDF_PARSER] é€£ç·šä¾‹å¤–: {e}")
        return None

def extract_metadata_via_llm(raw_text):
    """
    å‘¼å« VLLM API é€²è¡Œ Metadata æå–
    """
    url = f"{VLLM_ENDPOINT}/v1/chat/completions"
    
    # è¨­ç½®è«‹æ±‚æ¨™é ­ (åƒè€ƒæ‚¨æä¾›çš„ç¨‹å¼ç¢¼)
    headers = {"Content-Type": "application/json"}
    if BEARER_TOKEN:
        headers["Authorization"] = f"Bearer {BEARER_TOKEN}"

    # å®šç¾©æç¤ºè© (System Prompt + User Context)
    system_prompt = """
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªæ–‡ä»¶åˆ†æå¸«ã€‚è«‹åˆ†æä½¿ç”¨è€…æä¾›çš„ OCR æ–‡å­—ï¼Œæå–ä»¥ä¸‹ JSON æ¬„ä½ã€‚
    å¦‚æœæ‰¾ä¸åˆ°å°æ‡‰è³‡è¨Šï¼Œè«‹å¡« null æˆ–ç©ºé™£åˆ— []ã€‚
    
    å¿…é ˆæå–çš„æ¬„ä½:
    1. product_name (å­—ä¸²): ç”¢å“å…¨å
    2. product_code (å­—ä¸²): æ–‡è™Ÿæˆ–å•†å“ä»£ç¢¼
    3. insurance_type (å­—ä¸²é™£åˆ—): ä¾‹å¦‚ ["çµ‚èº«å£½éšª", "ç¾å…ƒä¿å–®", "åˆ©ç‡è®Šå‹•å‹"]
    4. target_audience (å­—ä¸²): é©åˆçš„å°è±¡æè¿°
    5. benefits (å­—ä¸²é™£åˆ—): ä¸»è¦çµ¦ä»˜é …ç›®
    6. currency (å­—ä¸²): å¹£åˆ¥ (å¦‚ USD, TWD)

    è«‹ç›´æ¥å›å‚³ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡‹æˆ– Markdown æ ¼å¼ã€‚
    """

    # ç‚ºäº†é¿å…è¶…é Token ä¸Šé™ï¼Œæˆ‘å€‘åªå–å‰ 3000 å€‹å­— (é€šå¸¸ metadata éƒ½åœ¨å‰å¹¾é )
    truncated_text = raw_text[:3000]

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"é€™æ˜¯ä¿éšªæ–‡ä»¶çš„ OCR å…§å®¹:\n\n{truncated_text}"}
        ],
        "temperature": 0.1, # ä½æº«ä»¥ç¢ºä¿è¼¸å‡ºæ ¼å¼ç©©å®š
        "max_tokens": 1024
    }

    try:
        print(f"[Python] Calling LLM: {url}...", file=sys.stderr)
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            resp_json = response.json()
            content = resp_json["choices"][0]["message"]["content"]
            
            # æ¸…ç†ä¸¦è§£æ JSON
            cleaned_content = clean_json_string(content)
            print(f"[Python] LLM Response: {cleaned_content}...", file=sys.stderr) # debug log
            
            return json.loads(cleaned_content)
        else:
            print(f"[Error] VLLM API Error: {response.status_code} - {response.text}", file=sys.stderr)
            return {}

    except Exception as e:
        print(f"[Error] Connection failed: {str(e)}", file=sys.stderr)
        return {}

def parse_pdf(file_path):
    """
    è®€å– PDFï¼Œç°¡å–®éæ¿¾é é¦–é å°¾ï¼Œå›å‚³çµæ§‹åŒ–è³‡æ–™ã€‚
    """
    result = {
        "file_path": file_path,
        "pages": [],
        "debug_info": [],
        "metadata": {}
    }

    full_text = ""
    
    try:
        with pdfplumber.open(file_path) as pdf:
            if len(pdf.pages) == 0:
                return json.dumps({"error": "PDF has 0 pages."})

            for i, page in enumerate(pdf.pages):
                # å…ˆæå–æ–‡å­—çœ‹çœ‹
                
                text = page.extract_text()
                method = "text_layer"

                # å¦‚æœæ–‡å­—å¤ªå°‘ (ä¾‹å¦‚ DM è½‰å¤–æ¡†)ï¼Œå‰‡å•Ÿå‹• OCR
                if not text or len(text.strip()) < 10:
                    try:
                        # å°‡é é¢è½‰ç‚ºåœ–ç‰‡ (è§£æåº¦ 300 dpi ä»¥æå‡è¾¨è­˜ç‡)
                        im = page.to_image(resolution=300).original
                        # ä½¿ç”¨ Tesseract è¾¨è­˜ç¹é«”ä¸­æ–‡ (chi_tra) + è‹±æ–‡ (eng)
                        text = pytesseract.image_to_string(im, lang='chi_tra+eng')
                        method = "ocr_fallback"
                    except Exception as e:
                        result["debug_info"].append(f"Page {i+1} OCR failed: {str(e)}")
                
                if text and len(text.strip()) > 0:
                    #ç°¡å–®æ¸…æ´—
                    clean_text = text.strip()

                    result["pages"].append({
                        "page_number": i + 1,
                        "content": clean_text,
                        "method": method # æ¨™è¨˜æ˜¯ç”¨ä»€éº¼æ–¹æ³•è®€åˆ°çš„
                    })
                    full_text += clean_text + "\n"

                else:
                    # è®€ä¸åˆ°æ–‡å­—ï¼Œè¨˜éŒ„åŸå› 
                    result["debug_info"].append(f"Page {i+1}: No text layer found. (Likely scanned image or encrypted)")
        
        # --- å‘¼å« LLM é€²è¡Œ Metadata æå– ---
        if len(full_text) > 0:
            # ç¢ºä¿æœ‰è¨­å®š Endpoint æ‰å‘¼å«
            if "YOUR_VLLM_IP" not in VLLM_ENDPOINT: 
                result["metadata"] = extract_metadata_via_llm(full_text)
            else:
                result["debug_info"].append("Skipped LLM call: VLLM_ENDPOINT not configured.")
                
    except Exception as e:
        return json.dumps({"error": str(e)})

    return json.dumps(result, ensure_ascii=False)
