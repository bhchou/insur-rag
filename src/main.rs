mod models;

use futures::TryStreamExt;
use dotenvy::dotenv; 
use serde_json::{Value, json};
use walkdir::WalkDir;
use sha2::{Sha256, Digest};
use hex;

use std::collections::{HashMap, HashSet};
use std::env; 
use std::path::Path;
use std::process::Command;
use std::sync::Arc;
use std::error::Error;
use std::thread;
use std::time::{self, Duration};
use std::fs;
use std::io::{self, Write};

use models::ParsedDocument;

// LanceDB èˆ‡ Arrow ç›¸é—œå¼•å…¥
use lancedb::{connect, query::{ExecutableQuery, QueryBase}};
use arrow_schema::{Schema, Field, DataType};
use arrow_array::{RecordBatch, RecordBatchIterator, StringArray, builder::Float32Builder, builder::FixedSizeListBuilder, Array};
use fastembed::{TextEmbedding, InitOptions, EmbeddingModel};

// --- è¨­å®šå€ ---
const RAW_PDF_DIR: &str = "./data/raw_pdfs"; // è«‹å»ºç«‹æ­¤è³‡æ–™å¤¾ä¸¦æ”¾å…¥æ‚¨çš„ 100 å€‹ PDF
const PROCESSED_JSON_DIR: &str = "./data/processed_json";
const DB_URI: &str = "data/lancedb_insure";
const TABLE_NAME: &str = "insurance_docs";
const SYNONYMS_PATH: &str = "./data/synonyms.json";

#[derive(Clone)]
struct ProductSummary {
    name: String,
    intro: String, // é€™è£¡æœƒå­˜ï¼šå•†å“é¡å‹ + ç‰¹è‰² + é©åˆå°è±¡
}

// è¼”åŠ©å‡½å¼ï¼šè¨ˆç®—å­—ä¸²çš„ SHA256 Hash
fn calculate_hash(content: &str) -> String {
    let mut hasher = Sha256::new();
    hasher.update(content.as_bytes());
    hex::encode(hasher.finalize())
}

// --- 1. Python Bridge (èˆ‡ Python æºé€š) ---
fn run_python_parser(pdf_path: &str) -> Result<ParsedDocument, Box<dyn Error>> {
    println!("ğŸ¦€ Rust: å‘¼å« Python è§£æå™¨è™•ç† {}...", pdf_path);

    let output = Command::new("python3")
        .arg("pysrc/pdf_parser.py") 
        .arg(pdf_path)
        .output()?;
    
    // ç„¡è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½æŠŠ Python çš„ Log å°å‡ºä¾†
    let stderr = String::from_utf8_lossy(&output.stderr);
    if !stderr.is_empty() {
        println!("ğŸ Python Debug Log:\n{}", stderr);
    }


    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python åŸ·è¡Œå¤±æ•—: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    
    // å¾ stdout ä¸­æŠ“å– JSON å­—ä¸² (éæ¿¾æ‰ Log)
    let json_str = find_json_part(&stdout).ok_or("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON")?;

    println!("ğŸ¦€ Rust: æ”¶åˆ° JSONï¼Œæ­£åœ¨è½‰æ›ç‚ºçµæ§‹é«”...");

    // å˜—è©¦è§£æï¼Œå¦‚æœå¤±æ•—ï¼Œå°±å°å‡ºé‚£ä¸²å®³æ­»ç¨‹å¼çš„ JSON
    let parsed_doc: ParsedDocument = match serde_json::from_str(json_str) {
        Ok(doc) => doc,
        Err(e) => {
            eprintln!("âŒ JSON è§£æå¤±æ•—ï¼éŒ¯èª¤åŸå› : {}", e);
            eprintln!("ğŸ“œ åŸå§‹ JSON å…§å®¹:\n{}", json_str); // è®“å…‡æ‰‹ç¾å½¢
            return Err(Box::new(e));
        }
    };

    // let parsed_doc: ParsedDocument = serde_json::from_str(json_str)?;

    Ok(parsed_doc)
}

// è¼”åŠ©å‡½å¼ï¼šæŠ“å‡º JSONå€å¡Š
fn find_json_part(text: &str) -> Option<&str> {
    let start = text.find('{')?;
    let end = text.rfind('}')?;
    if start <= end {
        Some(&text[start..=end])
    } else {
        None
    }
}

// --- 4. å‘é‡æœå°‹ (Retrieval), æš«æ™‚ä¸ç”¨ ---
async fn search_document(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    query_text: &str
) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ” æ­£åœ¨æœå°‹: \"{}\"", query_text);

    // 1. å°‡æŸ¥è©¢èªå¥è½‰ç‚ºå‘é‡

    let query_embedding = model.embed(vec![query_text.to_string()], None)?;
    let query_vector = query_embedding[0].clone(); // æ‹¿ç¬¬ä¸€ç­†(ä¹Ÿæ˜¯å”¯ä¸€ä¸€ç­†)

    // 2. é–‹å•Ÿ Table
    let table = db.open_table("insurance_docs").execute().await?;

    // 3. åŸ·è¡Œå‘é‡æœå°‹ (Vector Search)

    let results = table
        .query()
        .nearest_to(query_vector)? // å‚³å…¥ query å‘é‡
        .limit(3)
        .execute()
        .await?;

    // 4. è§£æä¸¦é¡¯ç¤ºçµæœ

    use futures::TryStreamExt;
    let batches: Vec<RecordBatch> = results.try_collect().await?;

    println!("--------------------------------------------------");
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        // LanceDB æœå°‹çµæœæœƒè‡ªå‹•å¤šä¸€å€‹ "_distance" æ¬„ä½ï¼Œä»£è¡¨ç›¸ä¼¼åº¦è·é›¢ (è¶Šå°è¶Šç›¸ä¼¼)
        // å¦‚æœ LanceDB ç‰ˆæœ¬è¼ƒèˆŠï¼Œå¯èƒ½æ²’æœ‰å›å‚³ distanceï¼Œé€™é‚Šå…ˆåšå€‹é˜²å‘†
        // let dist_col = batch.column_by_name("_distance"); 

        for i in 0..batch.num_rows() {
            let content = text_col.value(i);
            // é€™è£¡åšå­—ä¸²æˆªæ–·ï¼Œé¿å…å°å‡ºå¤ªå¤š
            let display_content: String = content.chars().take(100).collect();
            
            println!("ğŸ“„ [çµæœ {}]: {}...", i + 1, display_content);
            println!("--------------------------------------------------");
        }
    }

    Ok(())
}

// Semantic Chunking (æ ¸å¿ƒé‚è¼¯ï¼šæ³¨å…¥ Metadata) ---
fn semantic_chunking(doc: &ParsedDocument, filename: &str) -> Vec<String> {
    let mut chunks = Vec::new();
    let metadata = &doc.metadata;
    
    // å…ˆç”¨ç°¡å–®çš„å¥é»åˆ‡åˆ†
    let raw_sentences: Vec<&str> = doc.full_text.split("ã€‚").collect();

    for sentence in raw_sentences {
        let clean_text = sentence.trim();
        if clean_text.is_empty() { continue; }
        
        // å°‡å•†å“åç¨±èˆ‡æ–‡è™Ÿã€Œç„Šæ­»ã€åœ¨æ¯ä¸€æ®µæ–‡å­—å‰
        // é€™æ¨£ Embedding ä¹‹å¾Œï¼Œé€™æ®µå‘é‡å°±æ°¸é å¸¶æœ‰é€™äº›å±¬æ€§
        let enriched_chunk = format!(
            "ä¾†æº: {} | å•†å“: {} | æ–‡è™Ÿ: {} | å°è±¡: {} | å…§å®¹: {}", // åŠ å…¥å°è±¡
            filename,
            metadata.product_name, 
            metadata.product_code.clone().unwrap_or_default(), // è™•ç† Option
            metadata.target_audience.clone().unwrap_or("ä¸é™".to_string()), // è™•ç† Option
            clean_text
        );
        chunks.push(enriched_chunk);
    }
    
    // ç‰¹æ®Šè™•ç†ï¼šæŠŠ Benefit ä¹Ÿè®Šæˆç¨ç«‹çš„ Chunk
    for benefit in &metadata.benefits {
        let benefit_chunk = format!(
            "å•†å“: {} | çµ¦ä»˜é …ç›®: {}", 
            metadata.product_name, 
            benefit
        );
        chunks.push(benefit_chunk);
    }

    chunks
}

// --- 5. ç”Ÿæˆå›ç­” (Generation) ---
async fn ask_llm(context: &str, query: &str) -> Result<(), Box<dyn Error>> {
    println!("ğŸ¤– æ­£åœ¨è©¢å• LLM (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...");

    // 1. æº–å‚™ Prompt (ğŸ”¥ å·²å‡ç´šï¼šåŠ å…¥ä¾†æºå¼•ç”¨æŒ‡ä»¤)
    // æˆ‘å€‘å‘Šè¨´ LLMï¼Œå¦‚æœ context è£¡æœ‰æª”åï¼Œç›¡é‡åœ¨å›ç­”æ™‚å¸¶å‡ºä¾†
    let system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€(åŒ…å«å•†å“æ‘˜è¦èˆ‡è©³ç´°ç‰‡æ®µ) å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚\
    \n\né‡è¦è¦å‰‡ï¼š\
    \n1. è‹¥è³‡æ–™ä¸­åŒ…å«ä¾†æºæª”æ¡ˆåç¨± (Source File)ï¼Œè«‹å˜—è©¦åœ¨å›ç­”ä¸­æ¨™è¨»ã€‚\
    \n2. å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•å›ç­”ã€ï¼Œä¸è¦æé€ äº‹å¯¦ã€‚";

    let user_prompt = format!(
        "åƒè€ƒè³‡æ–™ï¼š\n{}\n\nä½¿ç”¨è€…å•é¡Œï¼š{}", 
        context, query
    );

    // 2. æº–å‚™ HTTP Client (ä¿ç•™æ‚¨çš„ no_proxy è¨­å®š)
    let client = reqwest::Client::builder()
        .no_proxy() // ä¸è¦ç®¡ http_proxy/HTTP_PROXY
        .build()?; 
    
    // è®€å–åŸå§‹çš„ç’°å¢ƒè®Šæ•¸
    let vllm_endpoint = env::var("VLLM_ENDPOINT")
        .unwrap_or("http://localhost:11434".to_string());
    
    // é è¨­æ¨¡å‹æ”¹ç‚ºæ‚¨å¯èƒ½ä½¿ç”¨çš„ (e.g., llama3, gemma2)
    let model_name = env::var("MODEL_NAME")
        .unwrap_or("llama3.1".to_string()); 
        
    let token = env::var("BEARER_TOKEN").unwrap_or_default();
    
    // è™•ç† URL çµå°¾
    let base_url = vllm_endpoint.trim_end_matches('/'); 
    
    // è‡ªå‹•åˆ¤æ–·æ˜¯å¦è£œä¸Š /v1/chat/completions
    let api_url = if base_url.contains("/v1") {
        format!("{}/chat/completions", base_url)
    } else {
        format!("{}/v1/chat/completions", base_url)
    };

    println!("ğŸ”— é€£ç·š Endpoint: {}", api_url);
    
    // ç™¼é€è«‹æ±‚ (OpenAI Compatible API æ ¼å¼)
    let body = json!({
        "model": model_name, 
        "messages": [
            { "role": "system", "content": system_prompt },
            { "role": "user", "content": user_prompt }
        ],
        "temperature": 0.1, // RAG å»ºè­°ä½æº«ï¼Œæ¸›å°‘å¹»è¦º
        "stream": false     // æ‚¨é¸æ“‡ä¸ä½¿ç”¨ä¸²æµ (é©åˆç°¡å–®è™•ç†)
    });

    let mut request_builder = client.post(&api_url)
        .header("Content-Type", "application/json")
        .header("User-Agent", "INSUR-RAG");

    // Token æª¢æŸ¥é‚è¼¯
    let token_check = token.trim().to_lowercase();
    let invalid_values = ["", "none", "null"];
    if !invalid_values.contains(&token_check.as_str()) {
        request_builder = request_builder.header("Authorization", format!("Bearer {}", token));
    }

    let res = request_builder
        .json(&body)
        .send() 
        .await?;

    // è§£æå›æ‡‰
    if res.status().is_success() {
        let response_json: Value = res.json().await?;
        
        // æŠ“å– choices[0].message.content
        if let Some(content) = response_json["choices"][0]["message"]["content"].as_str() {
            println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", content);
        } else {
            println!("âš ï¸ LLM å›æ‡‰æ ¼å¼ç„¡æ³•è§£æ (å¯èƒ½ç„¡å…§å®¹): {:?}", response_json);
        }
    } else {
        println!("âŒ LLM è«‹æ±‚å¤±æ•—: Status {}", res.status());
        // å˜—è©¦å°å‡ºéŒ¯èª¤è¨Šæ¯å¹«åŠ©é™¤éŒ¯
        println!("Response: {}", res.text().await?);
    }

    Ok(())
}

// --- å–®æª”è™•ç†æ ¸å¿ƒé‚è¼¯ (Core Logic) ---
async fn process_single_file(
    path: &Path, 
    db: &lancedb::Connection, 
    model: &mut TextEmbedding
) -> Result<(), Box<dyn Error>> {
    let filename = path.file_name().unwrap().to_str().unwrap();
    let path_str = path.to_str().unwrap();

    println!("--------------------------------------------------");
    println!("ğŸš€ é–‹å§‹è™•ç†: {}", filename);

    // [Check Idempotency] æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    // ç°¡å–®æŸ¥è©¢ DB æœ‰æ²’æœ‰é€™å€‹ filename
    if let Ok(table) = db.open_table(TABLE_NAME).execute().await {
        // ä½¿ç”¨ SQL style filter
        let filter = format!("source_file = '{}'", filename);
        let count = table.count_rows(Some(filter)).await?;
        if count > 0 {
            println!("â© æª”æ¡ˆå·²å­˜åœ¨ ({} ç­†ç´€éŒ„)ï¼Œè·³éè™•ç†: {}", count, filename);
            return Ok(());
        }
    }

    // Python è§£æ
    let doc = match run_python_parser(path_str) {
        Ok(d) => d,
        Err(e) => {
            eprintln!("âŒ è§£æå¤±æ•— [{}]: {}", filename, e);
            return Ok(()); // å›å‚³ Ok è®“è¿´åœˆç¹¼çºŒ
        }
    };

    // åˆ‡åˆ†
    let chunks = semantic_chunking(&doc, filename);
    if chunks.is_empty() {
        println!("âš ï¸  æª”æ¡ˆå…§å®¹ç‚ºç©ºï¼Œè·³éã€‚");
        return Ok(());
    }

    // Embedding (æ”¹åˆ†æ‰¹è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”)
    println!("ğŸ§  å‘é‡åŒ– {} å€‹ç‰‡æ®µ...", chunks.len());
    let batch_size = 30; 
    let mut embeddings = Vec::with_capacity(chunks.len());
    // ä½¿ç”¨ chunks() é€²è¡Œåˆ‡åˆ†
    for (_i, batch) in chunks.chunks(batch_size).enumerate() {
        // è½‰æˆ Vec<String> å‚³çµ¦ model
        let batch_vec = batch.to_vec();
        
        // åŸ·è¡Œå‘é‡åŒ–
        let batch_embeddings = model.embed(batch_vec, None)?;
        embeddings.extend(batch_embeddings);

        // æ¯ä¸€æ‰¹è™•ç†å®Œç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œè®“ CPU é™æº«
        thread::sleep(time::Duration::from_millis(50)); 
        
        // print!(".") ä¾†é¡¯ç¤ºé€²åº¦ï¼Œflush stdout ç¢ºä¿çœ‹å¾—åˆ°
        use std::io::{self, Write};
        print!(".");
        io::stdout().flush().unwrap();
    }
    println!("\nâœ… å‘é‡åŒ–å®Œæˆ");

    // æº–å‚™ Arrow Batch
    let total_rows = chunks.len();
    let dim = 768;

    let schema = Arc::new(Schema::new(vec![
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            dim as i32
        ), true),
        Field::new("product_name", DataType::Utf8, false),
        Field::new("source_file", DataType::Utf8, false), // æ–°å¢æ¬„ä½
    ]));

    let text_array = StringArray::from(chunks.clone());
    let product_array = StringArray::from(vec![doc.metadata.product_name.clone(); total_rows]);
    let source_array = StringArray::from(vec![filename.to_string(); total_rows]); 

    let mut list_builder = FixedSizeListBuilder::new(Float32Builder::with_capacity(total_rows * dim), dim as i32);
    for vector in &embeddings {
        list_builder.values().append_slice(vector);
        list_builder.append(true);
    }
    let vector_array = list_builder.finish();

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(text_array),
            Arc::new(vector_array),
            Arc::new(product_array),
            Arc::new(source_array),
        ],
    )?;

    // å¯«å…¥ DB (Append æ¨¡å¼)
    let table_names = db.table_names().execute().await?;
    if table_names.contains(&TABLE_NAME.to_string()) {
        let table = db.open_table(TABLE_NAME).execute().await?;
        // é€™è£¡éœ€è¦ç”¨ iterator åŒ…èµ·ä¾†
        let batches = RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        table.add(Box::new(batches)).execute().await?;
    } 
    else {
        // ç¬¬ä¸€æ¬¡å»ºç«‹
        let batches = arrow_array::RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        db.create_table(TABLE_NAME, Box::new(batches)).execute().await?;
    }

    println!("âœ… å®Œæˆ: {}", filename);
    Ok(())
}

/* for JSON and then */

// è®€å–å–®ä¸€ JSON æª”æ¡ˆ
fn load_policy_json(path: &Path) -> Result<models::PolicyData, Box<dyn Error>> {
    let content = fs::read_to_string(path)?;
    // ä½¿ç”¨ serde_json è§£æ
    let data: models::PolicyData = serde_json::from_str(&content)?;
    Ok(data)
}

fn chunk_policy_data(data: &models::PolicyData) -> Vec<String> {
    let mut chunks = Vec::new();
    let pname = &data.basic_info.product_name;
    let fname = &data.source_filename;
    
    // ğŸ”¥ğŸ”¥ğŸ”¥ã€æ™ºæ…§æ¨™ç±¤ç³»çµ±ã€‘ğŸ”¥ğŸ”¥ğŸ”¥
    // æˆ‘å€‘æ ¹æ“š JSON æ¬„ä½è‡ªå‹•æ¨å°å‡ºä½¿ç”¨è€…å¯èƒ½æœƒæœçš„å£èªé—œéµå­—
    let mut tags = Vec::new();

    // 1. æ ¸å¿ƒåˆ†é¡ (æŠ•è³‡ vs å‚³çµ±)
    if data.investment.is_investment_linked {
        tags.push("æŠ•è³‡å‹ä¿å–®".to_string());
        tags.push("åŸºé‡‘ä¿å–®".to_string()); // ä¿—ç¨±
        tags.push("è®Šé¡ä¿éšª".to_string());
        tags.push("ç†è²¡å‹ä¿éšª".to_string());
        tags.push("é«˜é¢¨éšªé«˜å ±é…¬".to_string()); // ç‰¹å¾µ
    } else {
        tags.push("å‚³çµ±å‹ä¿å–®".to_string());
        tags.push("å›ºå®šåˆ©ç‡".to_string());
        tags.push("ä¿è­‰çµ¦ä»˜".to_string());
    }

    // 2. åŠŸèƒ½éœ€æ±‚ (å­˜éŒ¢ vs ä¿éšœ vs é€€ä¼‘)
    let type_desc = &data.basic_info.product_type;
    let cov_death = &data.coverage.death_benefit;
    let cov_maturity = &data.coverage.maturity_benefit;

    // åˆ¤æ–·æ˜¯å¦ç‚ºã€Œå„²è“„/é€€ä¼‘ã€å°å‘
    // å¦‚æœæœ‰ã€Œæ»¿æœŸé‡‘ã€ã€ã€Œç”Ÿå­˜é‡‘ã€æˆ–é¡å‹æ˜¯ã€Œå¹´é‡‘ã€
    if type_desc.contains("å¹´é‡‘") || cov_maturity.len() > 5 { 
        tags.push("é€€ä¼‘è¦åŠƒ".to_string());
        tags.push("é¤Šè€é‡‘".to_string());
        tags.push("å„²è“„éšª".to_string()); // é›–ç„¶ç¾åœ¨æ³•è¦å°‘ç”¨æ­¤è©ï¼Œä½†æ°‘çœ¾æ„›æœ
        tags.push("å­˜éŒ¢".to_string());
        tags.push("ç¾é‡‘æµ".to_string());
    }

    // åˆ¤æ–·æ˜¯å¦ç‚ºã€Œç´”ä¿éšœ/å£½éšªã€å°å‘
    if type_desc.contains("å£½éšª") || cov_death.len() > 5 {
        tags.push("å£½éšªä¿éšœ".to_string());
        tags.push("èº«æ•…è³ å„Ÿ".to_string());
        tags.push("ç•™æ„›çµ¦å®¶äºº".to_string()); // è¡ŒéŠ·ç”¨èª
        tags.push("è³‡ç”¢å‚³æ‰¿".to_string());   // é«˜è³‡ç”¢æ—ç¾¤é—œéµå­—
    }

    // 3. å¹£åˆ¥ç‰¹æ€§ (ç¾å…ƒ/å¤–å¹£)
    let currencies = &data.basic_info.currency;
    if currencies.iter().any(|c| c.contains("USD") || c.contains("ç¾å…ƒ")) {
        tags.push("ç¾å…ƒä¿å–®".to_string());
        tags.push("ç¾é‡‘ä¿å–®".to_string());
        tags.push("å¼·å‹¢è²¨å¹£".to_string());
    }
    if currencies.iter().any(|c| c != "TWD" && c != "æ–°å°å¹£") {
        tags.push("å¤–å¹£ä¿å–®".to_string());
        tags.push("è³‡ç”¢é…ç½®".to_string());
    }

    // 4. ç¹³è²»æ–¹å¼ (èº‰ç¹³/æœŸç¹³)
    let payment = &data.basic_info.payment_period;
    if payment.contains("èº‰") || payment.contains("ä¸€æ¬¡") {
        tags.push("èº‰ç¹³".to_string());
        tags.push("ä¸€æ¬¡ç¹³æ¸…".to_string());
        tags.push("å–®ç­†æŠ•è³‡".to_string());
    } else {
        tags.push("æœŸç¹³".to_string());
        tags.push("åˆ†æœŸç¹³è²»".to_string());
    }

    // 5. ç‰¹æ®Šæ—ç¾¤ (é«˜é½¡/å°å­©)
    let target = &data.rag_data.target_audience;
    if target.contains("65æ­²") || target.contains("é«˜é½¡") {
        tags.push("éŠ€é«®æ—ä¿å–®".to_string());
        tags.push("é«˜é½¡æŠ•ä¿".to_string());
    }
    if target.contains("å°å­©") || target.contains("å­å¥³") {
        tags.push("å…’ç«¥ä¿å–®".to_string());
        tags.push("æ•™è‚²åŸºé‡‘".to_string());
    }

    // å°‡åŸæœ¬çš„é—œéµå­—ä¹ŸåŠ é€²ä¾† (å»é‡)
    for kw in &data.rag_data.keywords {
        if !tags.contains(kw) {
            tags.push(kw.clone());
        }
    }

    // ç”Ÿæˆæ¨™ç±¤å­—ä¸²ï¼Œä¾‹å¦‚: "[TAGS: æŠ•è³‡å‹, åŸºé‡‘, ç¾å…ƒä¿å–®, é€€ä¼‘]"
    let tags_str = format!("[é—œéµå­—: {}]", tags.join(", "));
    // ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

    // ä¿®æ”¹ Headerï¼ŒæŠŠé€™äº›å¼·å¤§çš„æ¨™ç±¤åŸ‹é€²æ¯ä¸€å€‹å‘é‡ç‰‡æ®µ
    let header = format!("å•†å“: {} | ä¾†æº: {} | {}", pname, fname, tags_str);

    // --- ä»¥ä¸‹ Chunk ç”Ÿæˆé‚è¼¯ä¿æŒä¸è®Šï¼Œä½†å› ç‚º Header è®Šå¼·äº†ï¼Œæ‰€æœ‰ Chunk éƒ½è®Šå¼·äº† ---

    // Chunk 1: åŸºæœ¬è³‡è¨Š
    let chunk_basic = format!(
        "{} | [åŸºæœ¬è³‡è¨Š] æ–‡è™Ÿ: {} | é¡å‹: {} | ç¹³è²»: {} | å¹£åˆ¥: {:?} | æŠ•ä¿å¹´é½¡: {} | ä¿è²»é–€æª»: {}",
        header,
        data.basic_info.product_code,
        data.basic_info.product_type,
        data.basic_info.payment_period,
        data.basic_info.currency,
        data.conditions.age_range,
        data.conditions.premium_limit
    );
    chunks.push(chunk_basic);

    // Chunk 2: ä¿éšœå…§å®¹
    let chunk_cov = format!(
        "{} | [ä¿éšœå…§å®¹] èº«æ•…/å–ªè‘¬çµ¦ä»˜: {} | æ»¿æœŸ/ç¥å£½çµ¦ä»˜: {} | å…¶ä»–æ¬Šç›Š: {:?}",
        header,
        data.coverage.death_benefit,
        data.coverage.maturity_benefit,
        data.coverage.other_benefits
    );
    chunks.push(chunk_cov);

    // Chunk 3: æŠ•è³‡ç‰¹è‰²
    if data.investment.is_investment_linked {
        let chunk_inv = format!(
            "{} | [æŠ•è³‡ç‰¹è‰²] æ­¤ç‚ºæŠ•è³‡å‹ä¿å–®(åŸºé‡‘/å…¨å§”)ã€‚ç‰¹è‰²: {:?} | é¢¨éšª: {:?}",
            header,
            data.investment.features,
            data.investment.risks
        );
        chunks.push(chunk_inv);
    }

    // Chunk 4: è²»ç”¨
    let chunk_fee = format!("{} | [è²»ç”¨èªªæ˜] {}", header, data.conditions.fees_and_discounts);
    chunks.push(chunk_fee);

    // Chunk 5: å®¢ç¾¤
    let chunk_meta = format!("{} | [é©ç”¨å®¢ç¾¤] {} | é¡å¤–æ¨™ç±¤: {:?}", header, data.rag_data.target_audience, tags);
    chunks.push(chunk_meta);

    // Chunk 6: FAQ
    for faq in &data.rag_data.faq {
        let chunk_faq = format!("{} | [å¸¸è¦‹å•é¡Œ] Q: {} | A: {}", header, faq.q, faq.a);
        chunks.push(chunk_faq);
    }

    chunks
}
// å°‡ PolicyData åˆ‡åˆ†æˆå¸¶æœ‰èªæ„çš„æ–‡å­—ç‰‡æ®µ (Semantic Chunking)
fn chunk_policy_data_old(data: &models::PolicyData) -> Vec<String> {
    let mut chunks = Vec::new();
    let pname = &data.basic_info.product_name;
    let fname = &data.source_filename;
    
    // Helper: ç”¢ç”Ÿæ¨™æº–åŒ–çš„ Context Header
    // è®“æ¯ä¸€æ®µæ–‡å­—éƒ½çŸ¥é“è‡ªå·±å±¬æ–¼å“ªå€‹å•†å“
    let header = format!("å•†å“: {} | ä¾†æº: {}", pname, fname);

    // Chunk 1: åŸºæœ¬è³‡è¨Šèˆ‡æŠ•ä¿è¦å‰‡
    // åŒ…å«: å…¬å¸ã€å¹£åˆ¥ã€é¡å‹ã€å¹´é½¡ã€ä¿è²»é™åˆ¶
    let chunk_basic = format!(
        "{} | [åŸºæœ¬è³‡è¨Š] æ–‡è™Ÿ: {} | é¡å‹: {} | ç¹³è²»: {} | å¹£åˆ¥: {:?} | æŠ•ä¿å¹´é½¡: {} | ä¿è²»é–€æª»: {}",
        header,
        data.basic_info.product_code,
        data.basic_info.product_type,
        data.basic_info.payment_period,
        data.basic_info.currency,
        data.conditions.age_range,
        data.conditions.premium_limit
    );
    chunks.push(chunk_basic);

    // Chunk 2: ä¿éšœå…§å®¹
    // åŒ…å«: èº«æ•…ã€æ»¿æœŸã€å…¶ä»–
    let chunk_cov = format!(
        "{} | [ä¿éšœå…§å®¹] èº«æ•…/å–ªè‘¬çµ¦ä»˜: {} | æ»¿æœŸ/ç¥å£½çµ¦ä»˜: {} | å…¶ä»–æ¬Šç›Š: {:?}",
        header,
        data.coverage.death_benefit,
        data.coverage.maturity_benefit,
        data.coverage.other_benefits
    );
    chunks.push(chunk_cov);

    // Chunk 3: æŠ•è³‡ç‰¹è‰² (å¦‚æœæœ‰)
    if data.investment.is_investment_linked {
        let chunk_inv = format!(
            "{} | [æŠ•è³‡ç‰¹è‰²] æ˜¯å¦é€£çµæŠ•è³‡: æ˜¯ | ç‰¹è‰²: {:?} | é¢¨éšª: {:?}",
            header,
            data.investment.features,
            data.investment.risks
        );
        chunks.push(chunk_inv);
    }

    // Chunk 4: è²»ç”¨èˆ‡æŠ˜æ‰£
    let chunk_fee = format!(
        "{} | [è²»ç”¨èªªæ˜] {}",
        header,
        data.conditions.fees_and_discounts
    );
    chunks.push(chunk_fee);

    // Chunk 5: å®¢ç¾¤èˆ‡é—œéµå­— (è¼”åŠ©æœå°‹)
    let chunk_meta = format!(
        "{} | [é©ç”¨å®¢ç¾¤] {} | é—œéµå­—: {:?}",
        header,
        data.rag_data.target_audience,
        data.rag_data.keywords
    );
    chunks.push(chunk_meta);

    // Chunk 6~N: FAQ (é»ƒé‡‘è³‡æ–™)
    // æ¯ä¸€é¡Œ QA ç¨ç«‹æˆä¸€å€‹ Chunkï¼Œæœå°‹å‘½ä¸­ç‡æ¥µé«˜
    for faq in &data.rag_data.faq {
        let chunk_faq = format!(
            "{} | [å¸¸è¦‹å•é¡Œ] Q: {} | A: {}",
            header, faq.q, faq.a
        );
        chunks.push(chunk_faq);
    }

    chunks
}

// --- 2. è™•ç†å–®ä¸€æª”æ¡ˆæµç¨‹ (Embedding + DB Insert) ---
async fn process_and_index_json(
    path: &Path,
    table: &lancedb::Table,
    model: &mut TextEmbedding
) -> Result<(), Box<dyn Error>> {
    let filename = path.file_name().unwrap().to_str().unwrap();
    let content = fs::read_to_string(path)?;
    let current_hash = calculate_hash(&content);

    // 1. è®€å– JSON
    let policy = match load_policy_json(path) {
        Ok(p) => p,
        Err(e) => {
            eprintln!("âŒ JSON è§£æå¤±æ•— {}: {}", filename, e);
            return Ok(());
        }
    };
    // ğŸ”¥ğŸ”¥ğŸ”¥ã€DIFF æ ¸å¿ƒé‚è¼¯ã€‘ğŸ”¥ğŸ”¥ğŸ”¥
    // æŸ¥è©¢ DB ä¸­æ˜¯å¦å·²æœ‰æ­¤æª”æ¡ˆï¼Œä¸¦å–å‡ºå®ƒçš„ file_hash
    let where_clause = format!("source_file = '{}'", policy.source_filename);
    
    let results = table
        .query()
        .only_if(where_clause)
        .limit(1) // åªè¦æŸ¥ä¸€ç­†å°±çŸ¥é“æœ‰æ²’æœ‰
        .execute()
        .await?;

    let batches: Vec<RecordBatch> = results.try_collect().await?;
    
    if let Some(batch) = batches.first() {
        if batch.num_rows() > 0 {
            // DB è£¡æœ‰é€™å€‹æª”ï¼Œæª¢æŸ¥ Hash æ˜¯å¦ä¸€æ¨£
            if let Some(hash_col) = batch.column_by_name("file_hash") {
                 if let Some(str_array) = hash_col.as_any().downcast_ref::<StringArray>() {
                     let db_hash = str_array.value(0); // å–ç¬¬ä¸€åˆ—çš„ Hash
                     
                     if db_hash == current_hash {
                         println!("â© [è·³é] æª”æ¡ˆæœªè®Šæ›´: {}", filename);
                         return Ok(()); // Hash ä¸€æ¨£ï¼Œå®Œå…¨ä¸åšäº‹
                     } else {
                         println!("ğŸ”„ [æ›´æ–°] æª”æ¡ˆå…§å®¹å·²è®Šæ›´ (Hashä¸åŒ)ï¼Œé‡æ–°ç´¢å¼•: {}", filename);
                         // Hash ä¸åŒï¼Œç¨‹å¼æœƒç¹¼çºŒå¾€ä¸‹èµ°ï¼ŒåŸ·è¡Œåˆªé™¤èˆŠè³‡æ–™+å¯«å…¥æ–°è³‡æ–™
                     }
                 }
            }
        }
    }
    // ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
    // 2. åˆ‡åˆ†
    let chunks = chunk_policy_data(&policy);
    if chunks.is_empty() { return Ok(()); }

    println!("ğŸ”„ æ­£åœ¨ç´¢å¼•: {} (ç”¢ç”Ÿ {} å€‹å‘é‡ç‰‡æ®µ)", filename, chunks.len());

    // 3. å‘é‡åŒ– (Embedding)
    let embeddings = model.embed(chunks.clone(), None)?;

    // 4. æº–å‚™å¯«å…¥ LanceDB çš„è³‡æ–™
    let total_chunks = chunks.len();
    let embedding_dim = 768; // BGE-Base çš„ç¶­åº¦

    // å»ºæ§‹ Arrow Arrays
    let source_array = StringArray::from(vec![policy.source_filename.clone(); total_chunks]);
    let hash_array = StringArray::from(vec![current_hash; total_chunks]);
    let text_array = StringArray::from(chunks);
    
    // å»ºæ§‹å‘é‡ Array (Flattened list)
    let mut vector_builder = FixedSizeListBuilder::new(
        arrow_array::builder::Float32Builder::new(),
        embedding_dim as i32,
    );
    
    for vec in embeddings {
        for val in vec {
            vector_builder.values().append_value(val);
        }
        vector_builder.append(true);
    }
    let vector_array = vector_builder.finish();

    // å»ºç«‹ RecordBatch
    let schema = Arc::new(Schema::new(vec![
        Field::new("source_file", DataType::Utf8, false),
        Field::new("file_hash", DataType::Utf8, false),
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            embedding_dim as i32
        ), false),
    ]));

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(source_array),
            Arc::new(hash_array),
            Arc::new(text_array),
            Arc::new(vector_array),
        ],
    )?;

    // 5. å¯«å…¥ DB (å…ˆåˆªé™¤èˆŠçš„å†å¯«å…¥ï¼Œç¢ºä¿ä¸é‡è¤‡)
    // æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ç”¨ source_filename ä¾†åˆªé™¤ï¼Œé€™å°æ‡‰åˆ°åŸå§‹ PDF/DOCX æª”å
    let delete_filter = format!("source_file = '{}'", policy.source_filename);
    table.delete(&delete_filter).await?;

    let batches = RecordBatchIterator::new(vec![Ok(batch)], schema.clone());

    
    table.add(Box::new(batches)).execute().await?;

    Ok(())
}

// --- 3. å•ç­”é‚è¼¯ ---
async fn handle_user_query(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    user_query: &str,
    synonyms: &HashMap<String, String>,
    summaries: &HashMap<String, ProductSummary>
) -> Result<(), Box<dyn Error>> {

    // 0. å­—å…¸æ“´å……
    let mut final_query = user_query.to_string();
    for (slang, term) in synonyms {
        if user_query.contains(slang) {
            println!("ğŸ’¡ [å­—å…¸å‘½ä¸­] '{}' -> åŠ ä¸Š '{}'", slang, term);
            final_query.push_str(" ");
            final_query.push_str(term);
        }
    }

    // 1. å‘é‡åŒ–å•é¡Œ
    // let query_embedding = model.embed(vec![user_query.to_string()], None)?;
    // let query_vector = query_embedding[0].clone();
    let query_vec = model.embed(vec![final_query.clone()], None)?[0].clone();

    // 2. æœå°‹ DB
    let table = db.open_table(TABLE_NAME).execute().await?;
    let results = table
        .query()
        .nearest_to(query_vec)?
        .limit(10) // å–å‰ 3 å€‹æœ€ç›¸é—œçš„ç‰‡æ®µ
        .execute()
        .await?;
    
    let batches: Vec<RecordBatch> = results.try_collect().await?;

     // 3. æª¢æŸ¥çµæœ (ç°¡æ˜“ä¿¡å¿ƒæª¢æŸ¥: æœ‰æ²’æœ‰çµæœ)
    let has_results = !batches.is_empty() && batches[0].num_rows() > 0;

    let mut used_batches = batches;

    // 4. AI è£œæ•‘ (å¦‚æœæ²’çµæœ)
    if !has_results {
        println!("âš ï¸  åˆæ­¥æœå°‹ç„¡çµæœï¼Œå˜—è©¦ AI æ·±åº¦æ“´å……...");
        if let Some(ai_kw) = expand_query_with_ai(user_query).await {
            let ai_vec = model.embed(vec![ai_kw], None)?[0].clone();
            let ai_results = table.query().nearest_to(ai_vec)?.limit(3).execute().await?;
            used_batches = ai_results.try_collect().await?;
        }
    }

    // 5. çµ„è£ Context (åŒ…å«å•†å“æ‘˜è¦)
    let mut hit_files = HashSet::new();
    let mut snippets_text = String::new();

    println!("\nğŸ” [RAG æª¢ç´¢çµæœ]");
    for batch in &used_batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        let src_col = batch.column_by_name("source_file").unwrap().as_any().downcast_ref::<StringArray>().unwrap();

        for i in 0..batch.num_rows() {
            let src = src_col.value(i);
            let txt = text_col.value(i);
            hit_files.insert(src.to_string());
            snippets_text.push_str(&format!("ğŸ“„ [ç‰‡æ®µ] ä¾†æº: {}\nå…§å®¹: {}\n\n", src, txt));
            // println!("   ğŸ“„ ä¾†æº: {} \n   ğŸ“ å…§å®¹: {}\n   ---", src, text);
            
           // context_buffer.push_str(text);
           // context_buffer.push('\n');
           // if !sources.contains(&src.to_string()) {
           //     sources.push(src.to_string());
           // }
        }
    }

    /* if context_buffer.is_empty() {
        println!("âš ï¸  æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚");
        return Ok(());
    } */
    if hit_files.is_empty() {
        println!("âš ï¸  æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚");
        // é€™è£¡å¯ä»¥è€ƒæ…®å‘¼å« AI Expansion
        return Ok(());
    }

    // 6. æ³¨å…¥æ‘˜è¦ (Summary Injection)
    let mut final_context = String::new();
    final_context.push_str("=== ç›¸é—œå•†å“åŸºæœ¬ä»‹ç´¹ ===\n");
    for filename in &hit_files {
        if let Some(summary) = summaries.get(filename) {
            final_context.push_str(&format!("ğŸ“„ ä¾†æº: {}\n{}\n", filename, summary.intro));
        }
    }
    final_context.push_str("========================\n\n");
    final_context.push_str("=== è©³ç´°æª¢ç´¢ç‰‡æ®µ ===\n");
    final_context.push_str(&snippets_text);

    // 7. æœ€å¾Œç”Ÿæˆ
    ask_llm(&final_context, user_query).await?;
    
    println!("\nğŸ“š [ç³»çµ±åƒè€ƒä¾†æºæ–‡ä»¶]");
    let mut sorted_files: Vec<_> = hit_files.into_iter().collect();
    sorted_files.sort(); // æ’å€‹åºæ¯”è¼ƒå¥½çœ‹
    for (idx, filename) in sorted_files.iter().enumerate() {
        // å¦‚æœæœ‰æ‘˜è¦ï¼Œé †ä¾¿å°å‡ºå•†å“åç¨±ï¼Œæ›´æ¸…æ¥š
        if let Some(summary) = summaries.get(filename) {
            println!(" {}. {} ({})", idx + 1, summary.name, filename);
        } else {
            println!(" {}. {}", idx + 1, filename);
        }
    }
    println!("==================================");
    // println!("ğŸ¤– (LLM æœƒæ ¹æ“šä¸Šè¿° Context å›ç­”æ‚¨çš„å•é¡Œ: '{}')", user_query);
    // println!("ğŸ“š åƒè€ƒæ–‡ä»¶: {:?}", sources);

    Ok(())
}

fn load_product_summaries() -> HashMap<String, ProductSummary> {
    let mut summaries = HashMap::new();
    let walker = WalkDir::new(PROCESSED_JSON_DIR).into_iter();
    
    for entry in walker.filter_map(|e| e.ok()) {
        let path = entry.path();
        if path.extension().map_or(false, |e| e == "json") {
            if let Ok(content) = fs::read_to_string(path) {
                if let Ok(data) = serde_json::from_str::<models::PolicyData>(&content) {
                    
                    // ğŸ”¥ çµ„åˆå‡ºä¸€å€‹æœ€å¼·çš„ã€Œå•†å“å±¥æ­·ã€
                    let intro = format!(
                        "ã€å•†å“ç¸½è¦½ã€‘\nåç¨±: {}\né¡å‹: {}\nç‰¹è‰²: {:?}\né©åˆå°è±¡: {}\n",
                        data.basic_info.product_name,
                        data.basic_info.product_type,
                        data.investment.features, // å¦‚æœæ˜¯å‚³çµ±å‹é€™è£¡å¯èƒ½æ˜¯ç©ºï¼Œæ²’é—œä¿‚
                        data.rag_data.target_audience
                    );

                    summaries.insert(data.source_filename.clone(), ProductSummary {
                        name: data.basic_info.product_name,
                        intro,
                    });
                }
            }
        }
    }
    println!("ğŸ“š å·²å¿«å– {} ç­†å•†å“æ‘˜è¦", summaries.len());
    summaries
}

// --- Main Workflow ---
/*#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    dotenv().ok(); // è¼‰å…¥ç’°å¢ƒè®Šæ•¸

    // æº–å‚™è³‡æ–™åº« (Local File)
    let uri = DB_URI;
    let db = connect(uri).execute().await?;
    println!("ğŸ’¾ é€£ç·šè‡³ LanceDB: {}", uri);


    // æº–å‚™ Embedding æ¨¡å‹ (BGE-M3 æˆ– Base)
    let mut model = TextEmbedding::try_new(
        InitOptions::new(EmbeddingModel::BGEBaseENV15)
            .with_show_download_progress(true)
    )?;
    // å»ºç«‹åŸå§‹æª”æ¡ˆç›®éŒ„ (å¦‚æœä¸å­˜åœ¨)
    if !Path::new(RAW_PDF_DIR).exists() {
        std::fs::create_dir_all(RAW_PDF_DIR)?;
        println!("âš ï¸ è«‹å°‡ PDF æª”æ¡ˆæ”¾å…¥ {} è³‡æ–™å¤¾ä¸­", RAW_PDF_DIR);
    }

    // æƒæç›®éŒ„
    println!("ğŸ” æƒæç›®éŒ„: {} ...", RAW_PDF_DIR);
    let walker = WalkDir::new(RAW_PDF_DIR).into_iter();

    for entry in walker.filter_map(|e| e.ok()) {
        let path = entry.path();
        // åªè™•ç† .pdf å’Œ .docx æª”æ¡ˆ
        if path.extension().map_or(false, |ext| ext == "pdf" || ext == "docx") {
            // å‘¼å«è™•ç†å‡½å¼
            if let Err(e) = process_single_file(path, &db, &mut model).await {
                eprintln!("ğŸ’¥ åš´é‡éŒ¯èª¤ (Skipped): {:?}", e);
            }

            // è™•ç†å®Œä¸€å€‹æª”æ¡ˆï¼Œä¼‘æ¯ 200 æ¯«ç§’ 
            // è®“ OS æœ‰æ©Ÿæœƒé€²è¡Œ I/O Flush å’Œè¨˜æ†¶é«”å›æ”¶
            thread::sleep(Duration::from_millis(200));
        }
    }

    println!("\nğŸ‰ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼");

    println!("âœ¨ è³‡æ–™åº«å¯«å…¥å®Œæˆï¼Œç¨ç­‰ 1 ç§’ç¢ºä¿å¯«å…¥...");
    tokio::time::sleep(std::time::Duration::from_secs(1)).await;

    // --- æ¸¬è©¦æœå°‹ ---
    // é€™è£¡æ¨¡æ“¬ä½¿ç”¨è€…å•å•é¡Œ
    let user_query = "è‡»ç¾åˆ©ç¾å…ƒåˆ©ç‡å‹çµ‚èº«ä¿éšªçš„ä¸»è¦çµ¦ä»˜é …ç›®æœ‰å“ªäº›ï¼Ÿ";
    
    // å‘¼å«æˆ‘å€‘å‰›å‰›å¯«çš„æœå°‹å‡½å¼
    //search_document(&db, &mut model, user_query).await?;
    //
    // ç‚ºäº†æ–¹ä¾¿ï¼Œæˆ‘å€‘æŠŠ search_document çš„é‚è¼¯æ¬éä¾†ç›´æ¥åœ¨é€™è£¡æœ
    
    println!("\nğŸ” [Step 1] æ­£åœ¨æª¢ç´¢...");
    let query_embedding = model.embed(vec![user_query.to_string()], None)?;
    let table = db.open_table("insurance_docs").execute().await?;
    let results = table
        .query()
        .nearest_to(query_embedding[0].clone())?
        .limit(15)
        .execute()
        .await?;
        
    let batches: Vec<RecordBatch> = results.try_collect().await?;
    
    // çµ„è£ Context
    let mut context_buffer = String::new();
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        for i in 0..batch.num_rows() {
            context_buffer.push_str(text_col.value(i));
            context_buffer.push('\n'); // ç”¨æ›è¡Œåˆ†éš”
        }
    }
    // Debug Log
    println!("\nğŸ‘€ [Debug] çµ¦ LLM çš„ Context å…§å®¹é è¦½ (å‰ 500 å­—):\n--------------------------------------------------");
    println!("{}", context_buffer.chars().take(500).collect::<String>());
    println!("... (å…± {} å­—)", context_buffer.len());
    println!("--------------------------------------------------");

    // ç”Ÿæˆ (Generation)
    println!("\nğŸ§  [Step 2] æ­£åœ¨ç”Ÿæˆå›ç­”...");
    ask_llm(&context_buffer, user_query).await?; 

    Ok(())
}*/

fn load_synonyms() -> HashMap<String, String> {
    if let Ok(content) = fs::read_to_string(SYNONYMS_PATH) {
        // å‡è¨­ JSON æ ¼å¼æ˜¯ {"mapping": {"å£èª": "è¡“èª"}}ï¼Œé€™è£¡ç°¡åŒ–è™•ç†ç›´æ¥è®€ Map
        // å¦‚æœæ‚¨çš„ Python ç”¢å‡ºæ˜¯ç›´æ¥çš„ Dictï¼Œé€™æ¨£å¯«æ˜¯å°çš„
        if let Ok(map) = serde_json::from_str::<HashMap<String, String>>(&content) {
            println!("ğŸ“š è¼‰å…¥é›¢ç·šåŒç¾©è©å­—å…¸: {} ç­†", map.len());
            return map;
        } 
        // å¦‚æœ Python ç”¢å‡ºåŒ…å« "mapping" keyï¼Œè«‹æ”¹ç”¨ Value è§£æ
    }
    println!("âš ï¸ ç„¡æ³•è¼‰å…¥å­—å…¸ ({}). å°‡åªä½¿ç”¨åŸå­—ä¸²æœå°‹ã€‚", SYNONYMS_PATH);
    HashMap::new()
}

// --- LLM APIï¼šæ“´å……é—œéµå­— (Query Expansion) ---
async fn expand_query_with_ai(query: &str) -> Option<String> {
    let api_key = std::env::var("GOOGLE_API_KEY").ok()?;
    println!("ğŸ¤– [AI ä»‹å…¥] æ­£åœ¨è«‹æ±‚ Gemini åˆ†ææ„åœ–: '{}'...", query);
    
    let client = reqwest::Client::new();
    let prompt = format!("ä½¿ç”¨è€…æœå°‹: '{}'ã€‚è«‹è½‰æ›ç‚º3å€‹å°ç£ä¿éšªå°ˆæ¥­é—œéµå­—(å¦‚:è®Šé¡è¬èƒ½å£½éšª, æœˆæ’¥å›)ï¼Œç”¨ç©ºç™½åˆ†éš”ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚", query);

    let request_body = json!({
        "contents": [{ "parts": [{ "text": prompt }] }]
    });

    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={}", api_key);

    match client.post(&url).json(&request_body).send().await {
        Ok(resp) => {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(text) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                    let clean = text.trim().replace("\n", " ");
                    println!("âœ¨ AI å»ºè­°é—œéµå­—: {}", clean);
                    return Some(clean);
                }
            }
        }
        Err(_) => {}
    }
    None
}

// --- LLM APIï¼šæœ€çµ‚å›ç­” (RAG Generation) ---
async fn ask_llm_with_context(context: &str, question: &str) -> Result<(), Box<dyn Error>> {
    let api_key = std::env::var("GOOGLE_API_KEY").expect("GOOGLE_API_KEY not found");
    
    let client = reqwest::Client::new();
    let system_prompt = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šæä¾›çš„ã€å•†å“ä»‹ç´¹ã€‘èˆ‡ã€è©³ç´°ç‰‡æ®µã€‘å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚è‹¥è³‡æ–™ä¸è¶³è«‹èª å¯¦å‘ŠçŸ¥ã€‚";
    let full_prompt = format!("{}\n\nåƒè€ƒè³‡æ–™:\n{}\n\nä½¿ç”¨è€…å•é¡Œ: {}", system_prompt, context, question);

    let request_body = json!({
        "contents": [{ "parts": [{ "text": full_prompt }] }]
    });

    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={}", api_key);

    println!("ğŸ¤– æ­£åœ¨è©¢å• LLM (ç”Ÿæˆå›ç­”ä¸­)...");
    match client.post(&url).json(&request_body).send().await {
        Ok(resp) => {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(text) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                    println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", text);
                } else {
                    println!("âŒ LLM å›å‚³æ ¼å¼éŒ¯èª¤æˆ–ç„¡å…§å®¹");
                }
            } else {
                println!("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰");
            }
        }
        Err(e) => println!("âŒ API å‘¼å«å¤±æ•—: {}", e),
    }
    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    dotenv().ok();

    // 1. åˆå§‹åŒ– DB
    let db = connect(DB_URI).execute().await?;
    println!("ğŸ’¾ é€£ç·šè‡³è³‡æ–™åº«: {}", DB_URI);

    // å»ºç«‹ Table (å¦‚æœä¸å­˜åœ¨)
    // æ³¨æ„: é€™è£¡å®šç¾© Schema
    let embedding_dim = 768;
    let schema = Arc::new(Schema::new(vec![
        Field::new("source_file", DataType::Utf8, false),
        Field::new("file_hash", DataType::Utf8, false), // â˜… æ–°å¢é€™ä¸€æ¬„
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            embedding_dim
        ), false),
    ]));

    /* let table = db.create_table(TABLE_NAME, RecordBatchIterator::new(vec![], schema.clone()))
        .execute_if_not_exists()
        .await?;
        */

    let table_names = db.table_names().execute().await?;
    let table = if table_names.contains(&TABLE_NAME.to_string()) {
        println!("ğŸ“‚ è³‡æ–™è¡¨ '{}' å·²å­˜åœ¨ï¼Œé–‹å•Ÿä¸­...", TABLE_NAME);
        db.open_table(TABLE_NAME).execute().await?
    } 
    else {
        println!("âœ¨ è³‡æ–™è¡¨ '{}' ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸­...", TABLE_NAME);
        // å»ºç«‹ä¸€å€‹ç©ºçš„è¿­ä»£å™¨ä¾†åˆå§‹åŒ–è¡¨çµæ§‹
        let batches: Vec<Result<RecordBatch, arrow_schema::ArrowError>> = vec![]; 
        db.create_table(TABLE_NAME, RecordBatchIterator::new(batches, schema.clone()))
            .execute()
            .await?
    };

    // 2. åˆå§‹åŒ– Embedding æ¨¡å‹
    println!("ğŸ§  è¼‰å…¥ Embedding æ¨¡å‹ (BGE-Base)...");
    let mut model = TextEmbedding::try_new(InitOptions::new(EmbeddingModel::BGEBaseENV15))?;
    let synonyms = load_synonyms();           // <--- é€™è£¡ç”¢ç”Ÿ synonyms
    let summaries = load_product_summaries(); // <--- é€™è£¡ç”¢ç”Ÿ summaries

    // 3. æƒæä¸¦ç´¢å¼• JSON
    println!("\nğŸš€ é–‹å§‹ç´¢å¼• JSON è³‡æ–™å¤¾: {}", PROCESSED_JSON_DIR);
    if Path::new(PROCESSED_JSON_DIR).exists() {
        let walker = WalkDir::new(PROCESSED_JSON_DIR).into_iter();
        for entry in walker.filter_map(|e| e.ok()) {
            let path = entry.path();
            if path.extension().map_or(false, |e| e == "json") {
                // å‘¼å«ç´¢å¼•å‡½å¼
                if let Err(e) = process_and_index_json(path, &table, &mut model).await {
                    eprintln!("âŒ ç´¢å¼•éŒ¯èª¤: {:?}", e);
                }
            }
        }
    } else {
        println!("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° {} è³‡æ–™å¤¾ï¼Œè«‹ç¢ºèª Python è…³æœ¬æ˜¯å¦åŸ·è¡ŒæˆåŠŸã€‚", PROCESSED_JSON_DIR);
    }
    
    println!("\nâœ… æ‰€æœ‰è³‡æ–™ç´¢å¼•å®Œæˆï¼");

    // 4. äº’å‹•æ¨¡å¼
    println!("\nğŸ¤– ä¿éšª AI é¡§å• (RAG CLI) å·²å°±ç·’");
    println!("ğŸ’¡ è¼¸å…¥å•é¡Œ (ä¾‹å¦‚: 'å®‰è¯æ–°å‰æ˜Ÿæœ‰ä»€éº¼è²»ç”¨?' æˆ– 'exit' é›¢é–‹)");
    
    loop {
        print!("\nUser > ");
        io::stdout().flush()?;
        let mut input = String::new();
        if io::stdin().read_line(&mut input).is_ok() {
            let q = input.trim();
            if q.eq_ignore_ascii_case("exit") { break; }
            if q.is_empty() { continue; }

            handle_user_query(&db, &mut model, q, &synonyms, &summaries).await?;
        }
    }

    Ok(())
}

