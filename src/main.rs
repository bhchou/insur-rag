mod models;

use futures::TryStreamExt;
use dotenvy::{dotenv, from_path}; // 1. å¼•å…¥å¥—ä»¶
use pyo3::prelude::*;
use pyo3::types::PyTuple;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::env; // å¼•å…¥æ¨™æº–ç’°å¢ƒåº«
use std::fs;
use std::path::Path;

use std::process::Command;
use std::sync::Arc;
use std::error::Error;

use models::{InsuranceMetadata, ParsedDocument};

// LanceDB èˆ‡ Arrow ç›¸é—œå¼•å…¥
use lancedb::{connect, Table, query::{ExecutableQuery, QueryBase}};
use arrow_schema::{Schema, Field, DataType};
use arrow_array::{RecordBatch, RecordBatchIterator, StringArray, builder::Float32Builder, builder::FixedSizeListBuilder, Array};
//use lancedb::arrow::arrow_schema::{Schema, Field, DataType};
//use lancedb::arrow::array::{RecordBatch, StringArray, Float32Builder, FixedSizeListBuilder, Array};
//use lancedb::arrow::arrow_array::{self, RecordBatch, StringArray, Float32Builder, FixedSizeListBuilder, Array};
// Embedding
use fastembed::{TextEmbedding, InitOptions, EmbeddingModel};

use serde_json::json;

// --- 1. Python Bridge (èˆ‡ Python æºé€š) ---
fn run_python_parser(pdf_path: &str) -> Result<ParsedDocument, Box<dyn Error>> {
    println!("ğŸ¦€ Rust: å‘¼å« Python è§£æå™¨è™•ç† {}...", pdf_path);

    // é€™è£¡å‡è¨­æ‚¨çš„ python script æœƒåå‡ºåŒ…å« metadata å’Œ full_text çš„ JSON
    // å¦‚æœç›®å‰çš„ Python åªæœ‰å Metadataï¼Œæ‚¨å¯ä»¥æš«æ™‚ Mock full_textï¼Œæˆ–æ˜¯ä¿®æ”¹ Python 
    let output = Command::new("python3")
        .arg("pysrc/pdf_parser.py") // è«‹ç¢ºèªæª”å
        .arg(pdf_path)
        .output()?;
    
    // â˜…â˜…â˜… æ–°å¢é€™æ®µï¼šç„¡è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½æŠŠ Python çš„ Log å°å‡ºä¾† â˜…â˜…â˜…
    let stderr = String::from_utf8_lossy(&output.stderr);
    if !stderr.is_empty() {
        println!("ğŸ Python Debug Log:\n{}", stderr);
    }
    // â˜…â˜…â˜… çµæŸæ–°å¢ â˜…â˜…â˜…

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python åŸ·è¡Œå¤±æ•—: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    
    // å¾ stdout ä¸­æŠ“å– JSON å­—ä¸² (éæ¿¾æ‰ Log)
    let json_str = find_json_part(&stdout).ok_or("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON")?;

    println!("ğŸ¦€ Rust: æ”¶åˆ° JSONï¼Œæ­£åœ¨è½‰æ›ç‚ºçµæ§‹é«”...");
    
    // é€™è£¡ååºåˆ—åŒ–æˆæ‚¨åœ¨ models.rs å®šç¾©çš„çµæ§‹
    // æ³¨æ„ï¼šå¦‚æœæ‚¨çš„ Python ç›®å‰åªå›å‚³ Metadataï¼Œé€™è£¡è¦ç¨å¾®æ”¹ä¸€ä¸‹
    // å‡è¨­ Python å›å‚³çš„æ˜¯å®Œæ•´çš„ ParsedDocument (å« metadata å’Œ text)
    let parsed_doc: ParsedDocument = serde_json::from_str(json_str)?;

    Ok(parsed_doc)
}

// è¼”åŠ©å‡½å¼ï¼šæŠ“å‡º JSONå€å¡Š
fn find_json_part(text: &str) -> Option<&str> {
    let start = text.find('{')?;
    let end = text.rfind('}')?;
    if start <= end {
        Some(&text[start..=end])
    } else {
        None
    }
}

// --- 4. å‘é‡æœå°‹ (Retrieval) ---
async fn search_document(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    query_text: &str
) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ” æ­£åœ¨æœå°‹: \"{}\"", query_text);

    // 1. å°‡æŸ¥è©¢èªå¥è½‰ç‚ºå‘é‡
    // æ³¨æ„ï¼šmodel.embed æ¥å— Vec<String>ï¼Œæ‰€ä»¥è¦æŠŠå–®ä¸€æŸ¥è©¢åŒ…èµ·ä¾†
    let query_embedding = model.embed(vec![query_text.to_string()], None)?;
    let query_vector = query_embedding[0].clone(); // æ‹¿ç¬¬ä¸€ç­†(ä¹Ÿæ˜¯å”¯ä¸€ä¸€ç­†)

    // 2. é–‹å•Ÿ Table
    let table = db.open_table("insurance_docs").execute().await?;

    // 3. åŸ·è¡Œå‘é‡æœå°‹ (Vector Search)
    // æœå°‹æœ€ç›¸ä¼¼çš„ 3 ç­†è³‡æ–™
    let results = table
        .query()
        .nearest_to(query_vector)? // å‚³å…¥ query å‘é‡
        .limit(3)
        .execute()
        .await?;

    // 4. è§£æä¸¦é¡¯ç¤ºçµæœ
    // results æ˜¯ä¸€å€‹ Stream of RecordBatchï¼Œæˆ‘å€‘æŠŠå®ƒè’é›†èµ·ä¾†
    use futures::TryStreamExt;
    let batches: Vec<RecordBatch> = results.try_collect().await?;

    println!("--------------------------------------------------");
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        // LanceDB æœå°‹çµæœæœƒè‡ªå‹•å¤šä¸€å€‹ "_distance" æ¬„ä½ï¼Œä»£è¡¨ç›¸ä¼¼åº¦è·é›¢ (è¶Šå°è¶Šç›¸ä¼¼)
        // æ³¨æ„ï¼šå¦‚æœæ‚¨çš„ LanceDB ç‰ˆæœ¬è¼ƒèˆŠï¼Œå¯èƒ½æ²’æœ‰å›å‚³ distanceï¼Œé€™é‚Šå…ˆåšå€‹é˜²å‘†
        // let dist_col = batch.column_by_name("_distance"); 

        for i in 0..batch.num_rows() {
            let content = text_col.value(i);
            // é€™è£¡å¯ä»¥åšå­—ä¸²æˆªæ–·ï¼Œé¿å…å°å‡ºå¤ªå¤š
            let display_content: String = content.chars().take(100).collect();
            
            println!("ğŸ“„ [çµæœ {}]: {}...", i + 1, display_content);
            println!("--------------------------------------------------");
        }
    }

    Ok(())
}

// --- 2. Semantic Chunking (æ ¸å¿ƒé‚è¼¯ï¼šæ³¨å…¥ Metadata) ---
fn semantic_chunking(doc: &ParsedDocument) -> Vec<String> {
    let mut chunks = Vec::new();
    let metadata = &doc.metadata;
    
    // é€™è£¡ä½¿ç”¨ç°¡å–®çš„å¥é»åˆ‡åˆ†ï¼Œå¯¦å‹™ä¸Šå¯æ›æˆæ›´è°æ˜çš„ TextSplitter
    let raw_sentences: Vec<&str> = doc.full_text.split("ã€‚").collect();

    for sentence in raw_sentences {
        let clean_text = sentence.trim();
        if clean_text.is_empty() { continue; }
        
        // â˜…â˜…â˜… é—œéµï¼šå°‡å•†å“åç¨±èˆ‡æ–‡è™Ÿã€Œç„Šæ­»ã€åœ¨æ¯ä¸€æ®µæ–‡å­—å‰ â˜…â˜…â˜…
        // é€™æ¨£ Embedding ä¹‹å¾Œï¼Œé€™æ®µå‘é‡å°±æ°¸é å¸¶æœ‰é€™äº›å±¬æ€§
        let enriched_chunk = format!(
            "å•†å“: {} | æ–‡è™Ÿ: {} | å°è±¡: {} | å…§å®¹: {}", // åŠ å…¥å°è±¡
            metadata.product_name, 
            metadata.product_code.clone().unwrap_or_default(), // è™•ç† Option
            metadata.target_audience.clone().unwrap_or("ä¸é™".to_string()), // è™•ç† Option
            clean_text
        );
        chunks.push(enriched_chunk);
    }
    
    // ç‰¹æ®Šè™•ç†ï¼šæŠŠ Benefit ä¹Ÿè®Šæˆç¨ç«‹çš„ Chunk
    for benefit in &metadata.benefits {
        let benefit_chunk = format!(
            "å•†å“: {} | çµ¦ä»˜é …ç›®: {}", 
            metadata.product_name, 
            benefit
        );
        chunks.push(benefit_chunk);
    }

    chunks
}

// --- 5. ç”Ÿæˆå›ç­” (Generation) ---
async fn ask_llm(context: &str, query: &str) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ¤– æ­£åœ¨è©¢å• LLM (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...");

    // 1. æº–å‚™ Prompt
    // é€™æ˜¯æœ€ç¶“å…¸çš„ RAG Prompt æ¨¡æ¿
    let system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•å›ç­”ã€ï¼Œä¸è¦æé€ äº‹å¯¦ã€‚";
    let user_prompt = format!(
        "åƒè€ƒè³‡æ–™ï¼š\n{}\n\nä½¿ç”¨è€…å•é¡Œï¼š{}", 
        context, query
    );

    // 2. æº–å‚™ HTTP Client
    // let client = reqwest::Client::new();
    let client = reqwest::Client::builder()
        .no_proxy() // â˜… é—œéµï¼šå‘Šè¨´å®ƒä¸è¦ç®¡ http_proxy/HTTP_PROXY
        .build()?;  // æ³¨æ„é€™è£¡æœƒå›å‚³ Resultï¼Œæ‰€ä»¥è¦åŠ  ?
    
    // 1. å…ˆè®€å–åŸå§‹çš„ç’°å¢ƒè®Šæ•¸ (ä¾‹å¦‚ "http://172.17.116.182:13407")
    let vllm_endpoint = std::env::var("VLLM_ENDPOINT")
        .unwrap_or("http://localhost:11434".to_string());
    let model_name = std::env::var("MODEL_NAME")
        .unwrap_or("gemma2:27b".to_string());

    // 2. åŸ·è¡Œ Python é‚£æ®µé‚è¼¯ï¼šå»å°¾ + åˆ¤æ–·è·¯å¾‘
    let base_url = vllm_endpoint.trim_end_matches('/'); // å°æ‡‰ .rstrip('/')
    
    let api_url = if base_url.contains("/v1") {
        format!("{}/chat/completions", base_url)
    } else {
        format!("{}/v1/chat/completions", base_url)
    };

    println!("ğŸ”— é€£ç·š Endpoint: {}", api_url);
    // 3. ç™¼é€è«‹æ±‚ (OpenAI Compatible API æ ¼å¼)
    let body = json!({
        "model": model_name, // â˜…è«‹ç¢ºèªæ‚¨çš„ Model åç¨± (å¦‚ llama3, mistral)
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.1, // RAG å»ºè­°ä½æº«ï¼Œæ¸›å°‘å¹»è¦º
        "stream": false
    });

    let token = std::env::var("BEARER_TOKEN").unwrap_or_default();
    let mut request_builder = client.post(&api_url)
        .header("Content-Type", "application/json")
        .header("User-Agent", "INSUR-RAG");
    let token_check = token.trim().to_lowercase();
    let invalid_values = ["", "none", "null"];
    if !invalid_values.contains(&token_check.as_str()) {
        // åªæœ‰æœ‰æ•ˆæ™‚æ‰åŠ å…¥ Authorization
        // println!("ğŸ” Token æœ‰æ•ˆï¼Œå·²åŠ å…¥ Header"); // Debug ç”¨ï¼Œå¯æ‹¿æ‰
        request_builder = request_builder.header("Authorization", format!("Bearer {}", token));
    }

    let res = request_builder
        .json(&body)
        .send() // é€™è£¡æ‰çœŸæ­£ç™¼é€
        .await?;

/*    let res = client.post(&api_url)
        .header("Content-Type", "application/json")
        // å¦‚æœéœ€è¦ API Key (ä¾‹å¦‚ OpenAI/DeepSeek)ï¼Œå¯åœ¨æ­¤åŠ  .bearer_auth("sk-...")
        .json(&body)
        .send()
        .await?; */

    // 4. è§£æå›æ‡‰
    if res.status().is_success() {
        let response_json: Value = res.json().await?;
        // æŠ“å– choices[0].message.content
        if let Some(content) = response_json["choices"][0]["message"]["content"].as_str() {
            println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", content);
        } else {
            println!("âš ï¸ LLM å›æ‡‰æ ¼å¼ç„¡æ³•è§£æ: {:?}", response_json);
        }
    } else {
        println!("âŒ LLM è«‹æ±‚å¤±æ•—: Status {}", res.status());
        println!("Response: {}", res.text().await?);
    }

    Ok(())
}

// --- 3. Main Workflow ---
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    dotenvy::dotenv().ok(); // è¼‰å…¥ç’°å¢ƒè®Šæ•¸

    // A. æº–å‚™è³‡æ–™åº« (Local File)
    let uri = "data/lancedb_store";
    let db = connect(uri).execute().await?;
    println!("ğŸ’¾ é€£ç·šè‡³ LanceDB: {}", uri);

    // B. æº–å‚™ Embedding æ¨¡å‹ (BGE-M3 æˆ– Base)
    let mut model = TextEmbedding::try_new(
        InitOptions::new(EmbeddingModel::BGEBaseENV15)
            .with_show_download_progress(true)
    )?;

    // C. åŸ·è¡Œ ETL æµç¨‹
    let pdf_path = "./data/sample.pdf";
    
    // 1. Python è§£æ
    // è¨»ï¼šå¦‚æœæ‚¨çš„ models.rs é‚„æ²’å®šç¾© ParsedDocumentï¼Œè«‹å…ˆåªè®€ Metadataï¼ŒFullText æš«æ™‚ç”¨ fake data æ¸¬è©¦
    let doc = run_python_parser(pdf_path)?; 
    println!("âœ… è§£æå®Œæˆ: {}", doc.metadata.product_name);

    // 2. æ™ºèƒ½åˆ‡åˆ†
    let text_chunks = semantic_chunking(&doc);
    println!("ğŸ”ª åˆ‡åˆ†æˆ {} å€‹èªæ„å€å¡Š", text_chunks.len());

    if text_chunks.is_empty() {
        println!("âš ï¸ æ²’æœ‰å…§å®¹å¯å­˜ï¼ŒçµæŸç¨‹åºã€‚");
        return Ok(());
    }

    // 3. å‘é‡åŒ– (Batch Embedding)
    println!("ğŸ§  é–‹å§‹å‘é‡åŒ–...");
    let embeddings = model.embed(text_chunks.clone(), None)?;

    // 4. æº–å‚™ Arrow Data (é€™æ˜¯ LanceDB è¦æ±‚çš„æ ¼å¼)
    
    let total_rows = text_chunks.len();
    let dim = 768; // BGE-Base ç¶­åº¦
                   //
    // 4.1 å®šç¾© Schema
    let schema = Arc::new(Schema::new(vec![
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            dim as i32
        ), true),
        Field::new("product_name", DataType::Utf8, false),
    ]));

    // 4.2 æ§‹å»ºæ¬„ä½æ•¸æ“š
    //let total_rows = text_chunks.len();
    let text_array = StringArray::from(text_chunks.clone());
    let product_array = StringArray::from(vec![doc.metadata.product_name.clone(); total_rows]);
    
    // 4.3 è™•ç†å‘é‡æ•¸æ“š (æ‰å¹³åŒ– -> FixedSizeList)
    //let flat_vectors: Vec<f32> = embeddings.iter().flat_map(|v| v.clone()).collect();
    //let vector_array = FixedSizeListArray::from_iter_primitive::<Float32Type, _, _>(
    //    total_rows,
    //    768
    //);
    //
    // 3. å»ºç«‹å‘é‡æ¬„ä½ (ä½¿ç”¨ Builderï¼Œé€™æ˜¯ Arrow 53 æœ€ç©©çš„å¯«æ³•)
    let mut list_builder = FixedSizeListBuilder::new(
        Float32Builder::with_capacity(total_rows * dim),
        dim as i32
    );

    for vector in &embeddings {
        // vector æ˜¯ Vec<f32>ï¼Œç›´æ¥ append slice
        list_builder.values().append_slice(vector);
        list_builder.append(true);
    }
    let vector_array = list_builder.finish();

    // 4.4 çµ„åˆ Batch
    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(text_array),
            Arc::new(vector_array),
            Arc::new(product_array),
        ],
    )?;

    // 4.5 Claude èªªè¦é€™æ¨£åš
    let batches = RecordBatchIterator::new(
        vec![Ok(batch)],
        schema.clone(),
    );

    // 5. å¯«å…¥è³‡æ–™åº«
    let table_name = "insurance_docs";
    /*
    let table_exists = db.table_names().execute().await?.contains(&table_name.to_string());

    if table_exists {
        let table = db.open_table(table_name).execute().await?;
        table.add(Box::new(std::iter::once(Ok(batch)))).execute().await?;
        println!("â• æˆåŠŸè¿½åŠ è³‡æ–™åˆ°ç¾æœ‰ Table");
    } else {
        db.create_table(table_name, Box::new(std::iter::once(Ok(batch)))).execute().await?;
        println!("âœ¨ æˆåŠŸå»ºç«‹æ–° Table ä¸¦å¯«å…¥è³‡æ–™");
    }*/
    let table_names = db.table_names().execute().await?;
    
    if table_names.contains(&table_name.to_string()) {
        let table = db.open_table(table_name).execute().await?;
        // table.add(Box::new(std::iter::once(Ok(batch)))).execute().await?;
        // CLAUDE èªªè¦é€™æ¨£åš
        let add_batches = RecordBatchIterator::new(
            vec![Ok(RecordBatch::try_new(
                schema.clone(),
                vec![
                    Arc::new(StringArray::from(text_chunks.clone())),
                    Arc::new({
                        let mut lb = FixedSizeListBuilder::new(
                            Float32Builder::with_capacity(total_rows * dim),
                            dim as i32
                        );
                        for vector in &embeddings {
                            lb.values().append_slice(vector);
                            lb.append(true);
                        }
                        lb.finish()
                    }),
                    Arc::new(StringArray::from(vec![doc.metadata.product_name.clone(); total_rows])),
                ],
            )?)],
            schema.clone(),
        );
        table.add(Box::new(add_batches)).execute().await?;
        println!("â• æˆåŠŸè¿½åŠ è³‡æ–™åˆ°ç¾æœ‰ Table");
    } 
    else {
        // db.create_table(table_name, Box::new(std::iter::once(Ok(batch)))).execute().await?;
        // CLAUDE èªªè¦é€™æ¨£åš
        db.create_table(table_name, Box::new(batches)).execute().await?;
        println!("âœ¨ æˆåŠŸå»ºç«‹æ–° Table ä¸¦å¯«å…¥è³‡æ–™");
    }

    println!("âœ¨ è³‡æ–™åº«å¯«å…¥å®Œæˆï¼Œç¨ç­‰ 1 ç§’ç¢ºä¿å¯«å…¥...");
    tokio::time::sleep(std::time::Duration::from_secs(1)).await;

    // --- æ¸¬è©¦æœå°‹ ---
    // é€™è£¡æ¨¡æ“¬ä½¿ç”¨è€…å•å•é¡Œ
    let user_query = "é€™å¼µä¿å–®çš„èº«æ•…çµ¦ä»˜æ¢ä»¶æ˜¯ä»€éº¼ï¼Ÿ";
    
    // å‘¼å«æˆ‘å€‘å‰›å‰›å¯«çš„æœå°‹å‡½å¼
    // æ³¨æ„ï¼šmodel ä¹‹å‰æ˜¯ mutï¼Œé€™è£¡å‚³åƒè€ƒå³å¯
    //search_document(&db, &mut model, user_query).await?;
    //
    // 1. æª¢ç´¢ (Retrieval)
    // ç‚ºäº†æ–¹ä¾¿ï¼Œæˆ‘å€‘æŠŠ search_document çš„é‚è¼¯ç¨å¾®æ¬éä¾†ä¸€é»ï¼Œæˆ–è€…ç›´æ¥åœ¨é€™è£¡æœ
    // (é€™è£¡ç¤ºç¯„ç›´æ¥åœ¨ main å¯«ç°¡å–®ç‰ˆï¼Œé¿å…å¤§å¹…æ”¹å‹• search_document ç°½ç« )
    
    println!("\nğŸ” [Step 1] æ­£åœ¨æª¢ç´¢...");
    let query_embedding = model.embed(vec![user_query.to_string()], None)?;
    let table = db.open_table("insurance_docs").execute().await?;
    let results = table
        .query()
        .nearest_to(query_embedding[0].clone())?
        .limit(15)
        .execute()
        .await?;
        
    let batches: Vec<RecordBatch> = results.try_collect().await?;
    
    // 2. çµ„è£ Context
    let mut context_buffer = String::new();
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        for i in 0..batch.num_rows() {
            context_buffer.push_str(text_col.value(i));
            context_buffer.push('\n'); // ç”¨æ›è¡Œåˆ†éš”
        }
    }
    // â˜…â˜…â˜… åŠ å…¥é€™æ®µ Debug Log â˜…â˜…â˜…
    println!("\nğŸ‘€ [Debug] çµ¦ LLM çš„ Context å…§å®¹é è¦½ (å‰ 500 å­—):\n--------------------------------------------------");
    println!("{}", context_buffer.chars().take(500).collect::<String>());
    println!("... (å…± {} å­—)", context_buffer.len());
    println!("--------------------------------------------------");

    // 3. ç”Ÿæˆ (Generation)
    println!("\nğŸ§  [Step 2] æ­£åœ¨ç”Ÿæˆå›ç­”...");
    ask_llm(&context_buffer, user_query).await?;

    Ok(())
}

/*
fn main() -> PyResult<()> {
    // --- é™¤éŒ¯ç”¨ Start ---
    let cwd = std::env::current_dir().unwrap();
    println!("Cargo åŸ·è¡Œç•¶ä¸‹çš„è·¯å¾‘ (CWD): {:?}", cwd);
    println!(".env æª”æ¡ˆæ˜¯å¦å­˜åœ¨æ–¼æ­¤è·¯å¾‘: {}", std::path::Path::new(".env").exists());
    // --- é™¤éŒ¯ç”¨ End ---
    // // ç¨‹å¼ä¸€å•Ÿå‹•å°±è¼‰å…¥ .env
    // é€™è¡Œæœƒæ‰¾ç•¶å‰ç›®éŒ„ä¸‹çš„ .env æª”ï¼Œä¸¦å°‡å…¶å…§å®¹æ³¨å…¥ç³»çµ±ç’°å¢ƒè®Šæ•¸
    // dotenv().expect(".env file not found");
    //from_path(Path::new(".env")).expect("æ‰¾ä¸åˆ° .env æª”æ¡ˆï¼Œè«‹ç¢ºèªå®ƒå°±åœ¨ cargo run åŸ·è¡Œçš„ç›®éŒ„ä¸‹");
    match dotenv() {
        Ok(path) => println!("æˆåŠŸè¼‰å…¥ .env: {:?}", path),
        Err(e) => {
            eprintln!("CRITICAL ERROR: .env è¼‰å…¥å¤±æ•—ï¼");
            eprintln!("éŒ¯èª¤åŸå› : {:?}", e); // é€™è¡Œæœƒå‘Šè¨´æˆ‘å€‘çœŸç›¸
            std::process::exit(1);
        }
    }
    // åœ¨ Rust ç«¯æª¢æŸ¥ä¸€ä¸‹æœ‰æ²’æœ‰è®€åˆ°ï¼Œæ–¹ä¾¿é™¤éŒ¯
    let endpoint = env::var("VLLM_ENDPOINT").unwrap_or("æœªè¨­å®š".to_string());
    println!("æ­£åœ¨é€£æ¥ LLM Endpoint: {}", endpoint);

    // ç•¶ Rust å•Ÿå‹• Python æ™‚ï¼Œå› ç‚ºä¸Šé¢å·²ç¶“åŸ·è¡Œé dotenv()ï¼Œ
    // æ‰€ä»¥ Python è£¡çš„ os.environ["VLLM_ENDPOINT"] ä¹Ÿæœƒè‡ªå‹•æœ‰å€¼ï¼
    // æ‚¨ä¸éœ€è¦åœ¨ Python è£¡å†è£ python-dotenvã€‚


    // 1. è¨­å®šè¦è®€å–çš„ PDF è·¯å¾‘
    let pdf_path = "./data/sample.pdf";
    if !Path::new(pdf_path).exists() {
        println!("æ‰¾ä¸åˆ°æª”æ¡ˆ: {}, è«‹ç¢ºèª data ç›®éŒ„ä¸‹æœ‰ PDF", pdf_path);
        return Ok(());
    }

    // 2. è®€å– Python script å…§å®¹
    let py_app = fs::read_to_string("pysrc/pdf_parser.py")
        .expect("ç„¡æ³•è®€å– python script");

    // 3. å•Ÿå‹• Python è§£è­¯å™¨
    Python::with_gil(|py| {
        // è¼‰å…¥æˆ‘å€‘çš„ Python æ¨¡çµ„
        // é€™è£¡å°‡ python ç¨‹å¼ç¢¼ä½œç‚ºä¸€å€‹ module è¼‰å…¥ï¼Œåç¨±å–ç‚º "parser_mod"
        let module = PyModule::from_code(py, &py_app, "pdf_parser.py", "parser_mod")?;

        // å–å¾— parse_pdf å‡½å¼
        let parse_func = module.getattr("parse_pdf")?;

        println!("æ­£åœ¨ä½¿ç”¨ Python è§£æ PDF: {}", pdf_path);

        // å‘¼å«å‡½å¼ï¼Œå‚³å…¥åƒæ•¸ (Tuple å½¢å¼)
        let args = PyTuple::new(py, &[pdf_path]);
        let result: String = parse_func.call1(args)?.extract()?;

        // 4. åœ¨ Rust ç«¯è™•ç†çµæœ (JSON)
        let parsed_json: Value = serde_json::from_str(&result).unwrap();

        if let Some(error) = parsed_json.get("error") {
            println!("è§£æå¤±æ•—: {}", error);
        } 
        else {
            // æª¢æŸ¥æœ‰æ²’æœ‰ debug_info (è­¦å‘Šè¨Šæ¯)
            if let Some(debug_infos) = parsed_json["debug_info"].as_array() {
                for info in debug_infos {
                    println!("[è­¦å‘Š] {}", info.as_str().unwrap_or(""));
                }
            }

            // é¡¯ç¤ºé é¢å…§å®¹
            if let Some(pages) = parsed_json["pages"].as_array() {
                let page_count = pages.len();
                println!("è§£ææˆåŠŸï¼å…±è®€å– {} é ã€‚", page_count);
                
                if page_count > 0 {
                    let first_page = &pages[0];
                    let method = first_page["method"].as_str().unwrap_or("unknown");
                    let content = first_page["content"].as_str().unwrap_or("");
                    
                    println!("--- ç¬¬ 1 é é è¦½ (ä½¿ç”¨æ–¹æ³•: {}) ---", method);
                    // åªå°å‡ºå‰ 150 å€‹å­—é¿å…æ´—ç‰ˆ
                    let preview_len = std::cmp::min(content.chars().count(), 150);
                    let preview: String = content.chars().take(preview_len).collect();
                    println!("{}...", preview);
                }
            }
        }

        Ok(())
    })
}*/
