mod models;

use futures::TryStreamExt;
use dotenvy::dotenv; 
use serde_json::{Value, json};
use walkdir::WalkDir;

use std::env; 
use std::path::Path;
use std::process::Command;
use std::sync::Arc;
use std::error::Error;
use std::thread;
use std::time::{self, Duration};

use models::ParsedDocument;

// LanceDB èˆ‡ Arrow ç›¸é—œå¼•å…¥
use lancedb::{connect, query::{ExecutableQuery, QueryBase}};
use arrow_schema::{Schema, Field, DataType};
use arrow_array::{RecordBatch, StringArray, builder::Float32Builder, builder::FixedSizeListBuilder, Array};
use fastembed::{TextEmbedding, InitOptions, EmbeddingModel};

// --- è¨­å®šå€ ---
const RAW_PDF_DIR: &str = "./data/raw_pdfs"; // è«‹å»ºç«‹æ­¤è³‡æ–™å¤¾ä¸¦æ”¾å…¥æ‚¨çš„ 100 å€‹ PDF
const DB_URI: &str = "data/lancedb_store";
const TABLE_NAME: &str = "insurance_docs";

// --- 1. Python Bridge (èˆ‡ Python æºé€š) ---
fn run_python_parser(pdf_path: &str) -> Result<ParsedDocument, Box<dyn Error>> {
    println!("ğŸ¦€ Rust: å‘¼å« Python è§£æå™¨è™•ç† {}...", pdf_path);

    let output = Command::new("python3")
        .arg("pysrc/pdf_parser.py") 
        .arg(pdf_path)
        .output()?;
    
    // ç„¡è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½æŠŠ Python çš„ Log å°å‡ºä¾†
    let stderr = String::from_utf8_lossy(&output.stderr);
    if !stderr.is_empty() {
        println!("ğŸ Python Debug Log:\n{}", stderr);
    }


    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python åŸ·è¡Œå¤±æ•—: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    
    // å¾ stdout ä¸­æŠ“å– JSON å­—ä¸² (éæ¿¾æ‰ Log)
    let json_str = find_json_part(&stdout).ok_or("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON")?;

    println!("ğŸ¦€ Rust: æ”¶åˆ° JSONï¼Œæ­£åœ¨è½‰æ›ç‚ºçµæ§‹é«”...");

    // å˜—è©¦è§£æï¼Œå¦‚æœå¤±æ•—ï¼Œå°±å°å‡ºé‚£ä¸²å®³æ­»ç¨‹å¼çš„ JSON
    let parsed_doc: ParsedDocument = match serde_json::from_str(json_str) {
        Ok(doc) => doc,
        Err(e) => {
            eprintln!("âŒ JSON è§£æå¤±æ•—ï¼éŒ¯èª¤åŸå› : {}", e);
            eprintln!("ğŸ“œ åŸå§‹ JSON å…§å®¹:\n{}", json_str); // è®“å…‡æ‰‹ç¾å½¢
            return Err(Box::new(e));
        }
    };

    // let parsed_doc: ParsedDocument = serde_json::from_str(json_str)?;

    Ok(parsed_doc)
}

// è¼”åŠ©å‡½å¼ï¼šæŠ“å‡º JSONå€å¡Š
fn find_json_part(text: &str) -> Option<&str> {
    let start = text.find('{')?;
    let end = text.rfind('}')?;
    if start <= end {
        Some(&text[start..=end])
    } else {
        None
    }
}

// --- 4. å‘é‡æœå°‹ (Retrieval), æš«æ™‚ä¸ç”¨ ---
async fn search_document(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    query_text: &str
) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ” æ­£åœ¨æœå°‹: \"{}\"", query_text);

    // 1. å°‡æŸ¥è©¢èªå¥è½‰ç‚ºå‘é‡

    let query_embedding = model.embed(vec![query_text.to_string()], None)?;
    let query_vector = query_embedding[0].clone(); // æ‹¿ç¬¬ä¸€ç­†(ä¹Ÿæ˜¯å”¯ä¸€ä¸€ç­†)

    // 2. é–‹å•Ÿ Table
    let table = db.open_table("insurance_docs").execute().await?;

    // 3. åŸ·è¡Œå‘é‡æœå°‹ (Vector Search)

    let results = table
        .query()
        .nearest_to(query_vector)? // å‚³å…¥ query å‘é‡
        .limit(3)
        .execute()
        .await?;

    // 4. è§£æä¸¦é¡¯ç¤ºçµæœ

    use futures::TryStreamExt;
    let batches: Vec<RecordBatch> = results.try_collect().await?;

    println!("--------------------------------------------------");
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        // LanceDB æœå°‹çµæœæœƒè‡ªå‹•å¤šä¸€å€‹ "_distance" æ¬„ä½ï¼Œä»£è¡¨ç›¸ä¼¼åº¦è·é›¢ (è¶Šå°è¶Šç›¸ä¼¼)
        // å¦‚æœ LanceDB ç‰ˆæœ¬è¼ƒèˆŠï¼Œå¯èƒ½æ²’æœ‰å›å‚³ distanceï¼Œé€™é‚Šå…ˆåšå€‹é˜²å‘†
        // let dist_col = batch.column_by_name("_distance"); 

        for i in 0..batch.num_rows() {
            let content = text_col.value(i);
            // é€™è£¡åšå­—ä¸²æˆªæ–·ï¼Œé¿å…å°å‡ºå¤ªå¤š
            let display_content: String = content.chars().take(100).collect();
            
            println!("ğŸ“„ [çµæœ {}]: {}...", i + 1, display_content);
            println!("--------------------------------------------------");
        }
    }

    Ok(())
}

// Semantic Chunking (æ ¸å¿ƒé‚è¼¯ï¼šæ³¨å…¥ Metadata) ---
fn semantic_chunking(doc: &ParsedDocument, filename: &str) -> Vec<String> {
    let mut chunks = Vec::new();
    let metadata = &doc.metadata;
    
    // å…ˆç”¨ç°¡å–®çš„å¥é»åˆ‡åˆ†
    let raw_sentences: Vec<&str> = doc.full_text.split("ã€‚").collect();

    for sentence in raw_sentences {
        let clean_text = sentence.trim();
        if clean_text.is_empty() { continue; }
        
        // å°‡å•†å“åç¨±èˆ‡æ–‡è™Ÿã€Œç„Šæ­»ã€åœ¨æ¯ä¸€æ®µæ–‡å­—å‰
        // é€™æ¨£ Embedding ä¹‹å¾Œï¼Œé€™æ®µå‘é‡å°±æ°¸é å¸¶æœ‰é€™äº›å±¬æ€§
        let enriched_chunk = format!(
            "ä¾†æº: {} | å•†å“: {} | æ–‡è™Ÿ: {} | å°è±¡: {} | å…§å®¹: {}", // åŠ å…¥å°è±¡
            filename,
            metadata.product_name, 
            metadata.product_code.clone().unwrap_or_default(), // è™•ç† Option
            metadata.target_audience.clone().unwrap_or("ä¸é™".to_string()), // è™•ç† Option
            clean_text
        );
        chunks.push(enriched_chunk);
    }
    
    // ç‰¹æ®Šè™•ç†ï¼šæŠŠ Benefit ä¹Ÿè®Šæˆç¨ç«‹çš„ Chunk
    for benefit in &metadata.benefits {
        let benefit_chunk = format!(
            "å•†å“: {} | çµ¦ä»˜é …ç›®: {}", 
            metadata.product_name, 
            benefit
        );
        chunks.push(benefit_chunk);
    }

    chunks
}

// --- 5. ç”Ÿæˆå›ç­” (Generation) ---
async fn ask_llm(context: &str, query: &str) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ¤– æ­£åœ¨è©¢å• LLM (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...");

    // 1. æº–å‚™ Prompt
    let system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•å›ç­”ã€ï¼Œä¸è¦æé€ äº‹å¯¦ã€‚";
    let user_prompt = format!(
        "åƒè€ƒè³‡æ–™ï¼š\n{}\n\nä½¿ç”¨è€…å•é¡Œï¼š{}", 
        context, query
    );

    // 2. æº–å‚™ HTTP Client

    let client = reqwest::Client::builder()
        .no_proxy() // ä¸è¦ç®¡ http_proxy/HTTP_PROXY
        .build()?; 
    
    // è®€å–åŸå§‹çš„ç’°å¢ƒè®Šæ•¸
    let vllm_endpoint = env::var("VLLM_ENDPOINT")
        .unwrap_or("http://localhost:11434".to_string());
    let model_name = env::var("MODEL_NAME")
        .unwrap_or("gemma2:27b".to_string());
    let token = env::var("BEARER_TOKEN").unwrap_or_default();
    

    let base_url = vllm_endpoint.trim_end_matches('/'); // å°æ‡‰ .rstrip('/')
    
    let api_url = if base_url.contains("/v1") {
        format!("{}/chat/completions", base_url)
    } else {
        format!("{}/v1/chat/completions", base_url)
    };

    println!("ğŸ”— é€£ç·š Endpoint: {}", api_url);
    
    // ç™¼é€è«‹æ±‚ (OpenAI Compatible API æ ¼å¼)
    let body = json!({
        "model": model_name, 
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.1, // RAG å»ºè­°ä½æº«ï¼Œæ¸›å°‘å¹»è¦º
        "stream": false
    });

    let mut request_builder = client.post(&api_url)
        .header("Content-Type", "application/json")
        .header("User-Agent", "INSUR-RAG");
    let token_check = token.trim().to_lowercase();
    let invalid_values = ["", "none", "null"];
    if !invalid_values.contains(&token_check.as_str()) {
        request_builder = request_builder.header("Authorization", format!("Bearer {}", token));
    }

    let res = request_builder
        .json(&body)
        .send() 
        .await?;

    // è§£æå›æ‡‰
    if res.status().is_success() {
        let response_json: Value = res.json().await?;
        // æŠ“å– choices[0].message.content
        if let Some(content) = response_json["choices"][0]["message"]["content"].as_str() {
            println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", content);
        } else {
            println!("âš ï¸ LLM å›æ‡‰æ ¼å¼ç„¡æ³•è§£æ: {:?}", response_json);
        }
    } else {
        println!("âŒ LLM è«‹æ±‚å¤±æ•—: Status {}", res.status());
        println!("Response: {}", res.text().await?);
    }

    Ok(())
}

// --- å–®æª”è™•ç†æ ¸å¿ƒé‚è¼¯ (Core Logic) ---
async fn process_single_file(
    path: &Path, 
    db: &lancedb::Connection, 
    model: &mut TextEmbedding
) -> Result<(), Box<dyn Error>> {
    let filename = path.file_name().unwrap().to_str().unwrap();
    let path_str = path.to_str().unwrap();

    println!("--------------------------------------------------");
    println!("ğŸš€ é–‹å§‹è™•ç†: {}", filename);

    // [Check Idempotency] æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    // ç°¡å–®æŸ¥è©¢ DB æœ‰æ²’æœ‰é€™å€‹ filename
    if let Ok(table) = db.open_table(TABLE_NAME).execute().await {
        // ä½¿ç”¨ SQL style filter
        let filter = format!("source_file = '{}'", filename);
        let count = table.count_rows(Some(filter)).await?;
        if count > 0 {
            println!("â© æª”æ¡ˆå·²å­˜åœ¨ ({} ç­†ç´€éŒ„)ï¼Œè·³éè™•ç†: {}", count, filename);
            return Ok(());
        }
    }

    // Python è§£æ
    let doc = match run_python_parser(path_str) {
        Ok(d) => d,
        Err(e) => {
            eprintln!("âŒ è§£æå¤±æ•— [{}]: {}", filename, e);
            return Ok(()); // å›å‚³ Ok è®“è¿´åœˆç¹¼çºŒ
        }
    };

    // åˆ‡åˆ†
    let chunks = semantic_chunking(&doc, filename);
    if chunks.is_empty() {
        println!("âš ï¸  æª”æ¡ˆå…§å®¹ç‚ºç©ºï¼Œè·³éã€‚");
        return Ok(());
    }

    // Embedding (æ”¹åˆ†æ‰¹è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”)
    println!("ğŸ§  å‘é‡åŒ– {} å€‹ç‰‡æ®µ...", chunks.len());
    let batch_size = 30; 
    let mut embeddings = Vec::with_capacity(chunks.len());
    // ä½¿ç”¨ chunks() é€²è¡Œåˆ‡åˆ†
    for (_i, batch) in chunks.chunks(batch_size).enumerate() {
        // è½‰æˆ Vec<String> å‚³çµ¦ model
        let batch_vec = batch.to_vec();
        
        // åŸ·è¡Œå‘é‡åŒ–
        let batch_embeddings = model.embed(batch_vec, None)?;
        embeddings.extend(batch_embeddings);

        // æ¯ä¸€æ‰¹è™•ç†å®Œç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œè®“ CPU é™æº«
        thread::sleep(time::Duration::from_millis(50)); 
        
        // print!(".") ä¾†é¡¯ç¤ºé€²åº¦ï¼Œflush stdout ç¢ºä¿çœ‹å¾—åˆ°
        use std::io::{self, Write};
        print!(".");
        io::stdout().flush().unwrap();
    }
    println!("\nâœ… å‘é‡åŒ–å®Œæˆ");

    // æº–å‚™ Arrow Batch
    let total_rows = chunks.len();
    let dim = 768;

    let schema = Arc::new(Schema::new(vec![
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            dim as i32
        ), true),
        Field::new("product_name", DataType::Utf8, false),
        Field::new("source_file", DataType::Utf8, false), // æ–°å¢æ¬„ä½
    ]));

    let text_array = StringArray::from(chunks.clone());
    let product_array = StringArray::from(vec![doc.metadata.product_name.clone(); total_rows]);
    let source_array = StringArray::from(vec![filename.to_string(); total_rows]); 

    let mut list_builder = FixedSizeListBuilder::new(Float32Builder::with_capacity(total_rows * dim), dim as i32);
    for vector in &embeddings {
        list_builder.values().append_slice(vector);
        list_builder.append(true);
    }
    let vector_array = list_builder.finish();

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(text_array),
            Arc::new(vector_array),
            Arc::new(product_array),
            Arc::new(source_array),
        ],
    )?;

    // å¯«å…¥ DB (Append æ¨¡å¼)
    let table_names = db.table_names().execute().await?;
    if table_names.contains(&TABLE_NAME.to_string()) {
        let table = db.open_table(TABLE_NAME).execute().await?;
        // é€™è£¡éœ€è¦ç”¨ iterator åŒ…èµ·ä¾†
        let batches = arrow_array::RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        table.add(Box::new(batches)).execute().await?;
    } 
    else {
        // ç¬¬ä¸€æ¬¡å»ºç«‹
        let batches = arrow_array::RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        db.create_table(TABLE_NAME, Box::new(batches)).execute().await?;
    }

    println!("âœ… å®Œæˆ: {}", filename);
    Ok(())
}

// --- Main Workflow ---
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    dotenv().ok(); // è¼‰å…¥ç’°å¢ƒè®Šæ•¸

    // æº–å‚™è³‡æ–™åº« (Local File)
    let uri = "data/lancedb_store";
    let db = connect(uri).execute().await?;
    println!("ğŸ’¾ é€£ç·šè‡³ LanceDB: {}", uri);

    // æº–å‚™ Embedding æ¨¡å‹ (BGE-M3 æˆ– Base)
    let mut model = TextEmbedding::try_new(
        InitOptions::new(EmbeddingModel::BGEBaseENV15)
            .with_show_download_progress(true)
    )?;
    // å»ºç«‹åŸå§‹æª”æ¡ˆç›®éŒ„ (å¦‚æœä¸å­˜åœ¨)
    if !Path::new(RAW_PDF_DIR).exists() {
        std::fs::create_dir_all(RAW_PDF_DIR)?;
        println!("âš ï¸ è«‹å°‡ PDF æª”æ¡ˆæ”¾å…¥ {} è³‡æ–™å¤¾ä¸­", RAW_PDF_DIR);
    }

    // æƒæç›®éŒ„
    println!("ğŸ” æƒæç›®éŒ„: {} ...", RAW_PDF_DIR);
    let walker = WalkDir::new(RAW_PDF_DIR).into_iter();

    for entry in walker.filter_map(|e| e.ok()) {
        let path = entry.path();
        // åªè™•ç† .pdf æª”æ¡ˆ
        if path.extension().map_or(false, |ext| ext == "pdf") {
            // å‘¼å«è™•ç†å‡½å¼
            if let Err(e) = process_single_file(path, &db, &mut model).await {
                eprintln!("ğŸ’¥ åš´é‡éŒ¯èª¤ (Skipped): {:?}", e);
            }

            // è™•ç†å®Œä¸€å€‹æª”æ¡ˆï¼Œä¼‘æ¯ 200 æ¯«ç§’ 
            // è®“ OS æœ‰æ©Ÿæœƒé€²è¡Œ I/O Flush å’Œè¨˜æ†¶é«”å›æ”¶
            thread::sleep(Duration::from_millis(200));
        }
    }

    println!("\nğŸ‰ æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆï¼");

    println!("âœ¨ è³‡æ–™åº«å¯«å…¥å®Œæˆï¼Œç¨ç­‰ 1 ç§’ç¢ºä¿å¯«å…¥...");
    tokio::time::sleep(std::time::Duration::from_secs(1)).await;

    // --- æ¸¬è©¦æœå°‹ ---
    // é€™è£¡æ¨¡æ“¬ä½¿ç”¨è€…å•å•é¡Œ
    let user_query = "æœ‰å“ªäº›ä¿å–®æ˜¯é‡å°30æ­²ä»¥ä¸Šå¥³æ€§è¨­è¨ˆçš„çµ‚èº«å£½éšªï¼Ÿ";
    
    // å‘¼å«æˆ‘å€‘å‰›å‰›å¯«çš„æœå°‹å‡½å¼
    //search_document(&db, &mut model, user_query).await?;
    //
    // ç‚ºäº†æ–¹ä¾¿ï¼Œæˆ‘å€‘æŠŠ search_document çš„é‚è¼¯æ¬éä¾†ç›´æ¥åœ¨é€™è£¡æœ
    
    println!("\nğŸ” [Step 1] æ­£åœ¨æª¢ç´¢...");
    let query_embedding = model.embed(vec![user_query.to_string()], None)?;
    let table = db.open_table("insurance_docs").execute().await?;
    let results = table
        .query()
        .nearest_to(query_embedding[0].clone())?
        .limit(15)
        .execute()
        .await?;
        
    let batches: Vec<RecordBatch> = results.try_collect().await?;
    
    // çµ„è£ Context
    let mut context_buffer = String::new();
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        for i in 0..batch.num_rows() {
            context_buffer.push_str(text_col.value(i));
            context_buffer.push('\n'); // ç”¨æ›è¡Œåˆ†éš”
        }
    }
    // Debug Log
    println!("\nğŸ‘€ [Debug] çµ¦ LLM çš„ Context å…§å®¹é è¦½ (å‰ 500 å­—):\n--------------------------------------------------");
    println!("{}", context_buffer.chars().take(500).collect::<String>());
    println!("... (å…± {} å­—)", context_buffer.len());
    println!("--------------------------------------------------");

    // ç”Ÿæˆ (Generation)
    println!("\nğŸ§  [Step 2] æ­£åœ¨ç”Ÿæˆå›ç­”...");
    ask_llm(&context_buffer, user_query).await?; 

    Ok(())
}
