mod models;

use futures::TryStreamExt;
use dotenvy::dotenv; 
use serde_json::{Value, json};
use walkdir::WalkDir;
use sha2::{Sha256, Digest};
use hex;
use serde::{Serialize, Deserialize};

use std::collections::{HashMap, HashSet};
use std::env; 
use std::f32::consts::E;
use std::path::Path;
use std::process::Command;
use std::sync::Arc;
use std::error::Error;
use std::thread;
use std::time::{self, Duration};
use std::fs;
use std::io::{self, Write};

use models::ParsedDocument;

// LanceDB èˆ‡ Arrow ç›¸é—œå¼•å…¥
use lancedb::{connect, query::{ExecutableQuery, QueryBase}};
use arrow_schema::{Schema, Field, DataType};
use arrow_array::{RecordBatch, RecordBatchIterator, StringArray, builder::Float32Builder, builder::FixedSizeListBuilder, Array};
use fastembed::{TextEmbedding, InitOptions, EmbeddingModel};

// --- è¨­å®šå€ ---
const RAW_PDF_DIR: &str = "./data/raw_pdfs"; // è«‹å»ºç«‹æ­¤è³‡æ–™å¤¾ä¸¦æ”¾å…¥æ‚¨çš„ 100 å€‹ PDF
const PROCESSED_JSON_DIR: &str = "./data/processed_json";
const DB_URI: &str = "data/lancedb_insure";
const TABLE_NAME: &str = "insurance_docs";
const SYNONYMS_PATH: &str = "./data/synonyms.json";

#[derive(Clone)]
struct ProductSummary {
    name: String,
    intro: String, // é€™è£¡æœƒå­˜ï¼šå•†å“é¡å‹ + ç‰¹è‰² + é©åˆå°è±¡
}

// --- Rerank API çµæ§‹ ---
#[derive(Serialize)]
struct RerankRequest {
    query: String,
    documents: Vec<String>,
}

#[derive(Deserialize)]
struct RerankResponse {
    scores: Vec<f32>,
    indices: Vec<usize>,
}

// è¼”åŠ©å‡½å¼ï¼šè¨ˆç®—å­—ä¸²çš„ SHA256 Hash
fn calculate_hash(content: &str) -> String {
    let mut hasher = Sha256::new();
    hasher.update(content.as_bytes());
    hex::encode(hasher.finalize())
}

// --- 1. Python Bridge (èˆ‡ Python æºé€š) ---
fn run_python_parser(pdf_path: &str) -> Result<ParsedDocument, Box<dyn Error>> {
    println!("ğŸ¦€ Rust: å‘¼å« Python è§£æå™¨è™•ç† {}...", pdf_path);

    let output = Command::new("python3")
        .arg("pysrc/pdf_parser.py") 
        .arg(pdf_path)
        .output()?;
    
    // ç„¡è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½æŠŠ Python çš„ Log å°å‡ºä¾†
    let stderr = String::from_utf8_lossy(&output.stderr);
    if !stderr.is_empty() {
        println!("ğŸ Python Debug Log:\n{}", stderr);
    }


    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python åŸ·è¡Œå¤±æ•—: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    
    // å¾ stdout ä¸­æŠ“å– JSON å­—ä¸² (éæ¿¾æ‰ Log)
    let json_str = find_json_part(&stdout).ok_or("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON")?;

    println!("ğŸ¦€ Rust: æ”¶åˆ° JSONï¼Œæ­£åœ¨è½‰æ›ç‚ºçµæ§‹é«”...");

    // å˜—è©¦è§£æï¼Œå¦‚æœå¤±æ•—ï¼Œå°±å°å‡ºé‚£ä¸²å®³æ­»ç¨‹å¼çš„ JSON
    let parsed_doc: ParsedDocument = match serde_json::from_str(json_str) {
        Ok(doc) => doc,
        Err(e) => {
            eprintln!("âŒ JSON è§£æå¤±æ•—ï¼éŒ¯èª¤åŸå› : {}", e);
            eprintln!("ğŸ“œ åŸå§‹ JSON å…§å®¹:\n{}", json_str); // è®“å…‡æ‰‹ç¾å½¢
            return Err(Box::new(e));
        }
    };

    // let parsed_doc: ParsedDocument = serde_json::from_str(json_str)?;

    Ok(parsed_doc)
}

// è¼”åŠ©å‡½å¼ï¼šæŠ“å‡º JSONå€å¡Š
fn find_json_part(text: &str) -> Option<&str> {
    let start = text.find('{')?;
    let end = text.rfind('}')?;
    if start <= end {
        Some(&text[start..=end])
    } else {
        None
    }
}

// --- 4. å‘é‡æœå°‹ (Retrieval), æš«æ™‚ä¸ç”¨ ---
async fn search_document(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    query_text: &str
) -> Result<(), Box<dyn Error>> {
    println!("\nğŸ” æ­£åœ¨æœå°‹: \"{}\"", query_text);

    // 1. å°‡æŸ¥è©¢èªå¥è½‰ç‚ºå‘é‡

    let query_embedding = model.embed(vec![query_text.to_string()], None)?;
    let query_vector = query_embedding[0].clone(); // æ‹¿ç¬¬ä¸€ç­†(ä¹Ÿæ˜¯å”¯ä¸€ä¸€ç­†)

    // 2. é–‹å•Ÿ Table
    let table = db.open_table("insurance_docs").execute().await?;

    // 3. åŸ·è¡Œå‘é‡æœå°‹ (Vector Search)

    let results = table
        .query()
        .nearest_to(query_vector)? // å‚³å…¥ query å‘é‡
        .limit(3)
        .execute()
        .await?;

    // 4. è§£æä¸¦é¡¯ç¤ºçµæœ

    use futures::TryStreamExt;
    let batches: Vec<RecordBatch> = results.try_collect().await?;

    println!("--------------------------------------------------");
    for batch in batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        // LanceDB æœå°‹çµæœæœƒè‡ªå‹•å¤šä¸€å€‹ "_distance" æ¬„ä½ï¼Œä»£è¡¨ç›¸ä¼¼åº¦è·é›¢ (è¶Šå°è¶Šç›¸ä¼¼)
        // å¦‚æœ LanceDB ç‰ˆæœ¬è¼ƒèˆŠï¼Œå¯èƒ½æ²’æœ‰å›å‚³ distanceï¼Œé€™é‚Šå…ˆåšå€‹é˜²å‘†
        // let dist_col = batch.column_by_name("_distance"); 

        for i in 0..batch.num_rows() {
            let content = text_col.value(i);
            // é€™è£¡åšå­—ä¸²æˆªæ–·ï¼Œé¿å…å°å‡ºå¤ªå¤š
            let display_content: String = content.chars().take(100).collect();
            
            println!("ğŸ“„ [çµæœ {}]: {}...", i + 1, display_content);
            println!("--------------------------------------------------");
        }
    }

    Ok(())
}

// Semantic Chunking (æ ¸å¿ƒé‚è¼¯ï¼šæ³¨å…¥ Metadata) ---
fn semantic_chunking(doc: &ParsedDocument, filename: &str) -> Vec<String> {
    let mut chunks = Vec::new();
    let metadata = &doc.metadata;
    
    // å…ˆç”¨ç°¡å–®çš„å¥é»åˆ‡åˆ†
    let raw_sentences: Vec<&str> = doc.full_text.split("ã€‚").collect();

    for sentence in raw_sentences {
        let clean_text = sentence.trim();
        if clean_text.is_empty() { continue; }
        
        // å°‡å•†å“åç¨±èˆ‡æ–‡è™Ÿã€Œç„Šæ­»ã€åœ¨æ¯ä¸€æ®µæ–‡å­—å‰
        // é€™æ¨£ Embedding ä¹‹å¾Œï¼Œé€™æ®µå‘é‡å°±æ°¸é å¸¶æœ‰é€™äº›å±¬æ€§
        let enriched_chunk = format!(
            "ä¾†æº: {} | å•†å“: {} | æ–‡è™Ÿ: {} | å°è±¡: {} | å…§å®¹: {}", // åŠ å…¥å°è±¡
            filename,
            metadata.product_name, 
            metadata.product_code.clone().unwrap_or_default(), // è™•ç† Option
            metadata.target_audience.clone().unwrap_or("ä¸é™".to_string()), // è™•ç† Option
            clean_text
        );
        chunks.push(enriched_chunk);
    }
    
    // ç‰¹æ®Šè™•ç†ï¼šæŠŠ Benefit ä¹Ÿè®Šæˆç¨ç«‹çš„ Chunk
    for benefit in &metadata.benefits {
        let benefit_chunk = format!(
            "å•†å“: {} | çµ¦ä»˜é …ç›®: {}", 
            metadata.product_name, 
            benefit
        );
        chunks.push(benefit_chunk);
    }

    chunks
}

fn load_system_prompt() -> String {
    // 1. å˜—è©¦å¾ env è®€å–è·¯å¾‘
    let path = env::var("SYSTEM_PROMPT_PATH").unwrap_or("./data/system_prompt.txt".to_string());
    
    // 2. è®€å–æª”æ¡ˆå…§å®¹
    match fs::read_to_string(path.clone()) {
        Ok(content) => {
            println!("ğŸ“œ å·²è¼‰å…¥ System Prompt ({} bytes)", content.len());
            content
        },
        Err(e) => {
            println!("âš ï¸ ç„¡æ³•è®€å– Prompt æª”æ¡ˆ ({})ï¼Œä½¿ç”¨å…§å»ºé è¨­å€¼ã€‚éŒ¯èª¤: {}", path, e);
            // é€™è£¡æ”¾ä¸€å€‹æœ€ç°¡å–®çš„é è¨­å€¼ç•¶ä½œå‚™æ¡ˆ
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šåƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚".to_string()
        }
    }
}

// --- 5. ç”Ÿæˆå›ç­” (Generation) ---
async fn ask_llm(context: &str, query: &str) -> Result<(), Box<dyn Error>> {
    let system_prompt_text = load_system_prompt();
    println!("ğŸ¤– æ­£åœ¨è©¢å• LLM (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...");

    // 1. æº–å‚™ Prompt (ğŸ”¥ å·²å‡ç´šï¼šåŠ å…¥ä¾†æºå¼•ç”¨æŒ‡ä»¤)
    // æˆ‘å€‘å‘Šè¨´ LLMï¼Œå¦‚æœ context è£¡æœ‰æª”åï¼Œç›¡é‡åœ¨å›ç­”æ™‚å¸¶å‡ºä¾†
    let system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€(åŒ…å«å•†å“æ‘˜è¦èˆ‡è©³ç´°ç‰‡æ®µ) å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚\
    \n\né‡è¦è¦å‰‡ï¼š\
    \n1. è‹¥è³‡æ–™ä¸­åŒ…å«ä¾†æºæª”æ¡ˆåç¨± (Source File)ï¼Œè«‹å˜—è©¦åœ¨å›ç­”ä¸­æ¨™è¨»ã€‚\
    \n2. è‹¥è³‡æ–™ä¸­æœªæåŠå…·é«”ä¿é¡å»ºè­°ï¼Œè«‹æ ¹æ“šä¿éšªå­¸ç†ï¼ˆå¦‚ï¼šé›™ååŸå‰‡ï¼‰æä¾›é€šç”¨çš„è²¡å‹™è¦åŠƒå»ºè­°ï¼Œä½†å¿…é ˆæ¨™è¨»ã€æ­¤ç‚ºé€šç”¨å»ºè­°ã€ã€‚\
    \n3. å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•å›ç­”ã€ï¼Œä¸è¦æé€ äº‹å¯¦ã€‚";

    let user_prompt = format!(
        "åƒè€ƒè³‡æ–™ï¼š\n{}\n\nä½¿ç”¨è€…å•é¡Œï¼š{}", 
        context, query
    );

    // 2. æº–å‚™ HTTP Client (ä¿ç•™æ‚¨çš„ no_proxy è¨­å®š)
    let client = reqwest::Client::builder()
        .no_proxy() // ä¸è¦ç®¡ http_proxy/HTTP_PROXY
        .build()?; 
    
    // è®€å–åŸå§‹çš„ç’°å¢ƒè®Šæ•¸
    let vllm_endpoint = env::var("VLLM_ENDPOINT")
        .unwrap_or("http://localhost:11434".to_string());
    
    // é è¨­æ¨¡å‹æ”¹ç‚ºæ‚¨å¯èƒ½ä½¿ç”¨çš„ (e.g., llama3, gemma2)
    let model_name = env::var("MODEL_NAME")
        .unwrap_or("llama3.1".to_string()); 
        
    let token = env::var("BEARER_TOKEN").unwrap_or_default();
    
    // è™•ç† URL çµå°¾
    let base_url = vllm_endpoint.trim_end_matches('/'); 
    
    // è‡ªå‹•åˆ¤æ–·æ˜¯å¦è£œä¸Š /v1/chat/completions
    let api_url = if base_url.contains("/v1") {
        format!("{}/chat/completions", base_url)
    } else {
        format!("{}/v1/chat/completions", base_url)
    };

    println!("ğŸ”— é€£ç·š Endpoint: {}", api_url);
    
    // ç™¼é€è«‹æ±‚ (OpenAI Compatible API æ ¼å¼)
    let body = json!({
        "model": model_name, 
        "messages": [
            { "role": "system", "content": system_prompt_text },
            { "role": "user", "content": user_prompt }
        ],
        "temperature": 0.1, // RAG å»ºè­°ä½æº«ï¼Œæ¸›å°‘å¹»è¦º
        "stream": false     // æ‚¨é¸æ“‡ä¸ä½¿ç”¨ä¸²æµ (é©åˆç°¡å–®è™•ç†)
    });

    let mut request_builder = client.post(&api_url)
        .header("Content-Type", "application/json")
        .header("User-Agent", "INSUR-RAG");

    // Token æª¢æŸ¥é‚è¼¯
    let token_check = token.trim().to_lowercase();
    let invalid_values = ["", "none", "null"];
    if !invalid_values.contains(&token_check.as_str()) {
        request_builder = request_builder.header("Authorization", format!("Bearer {}", token));
    }

    let res = request_builder
        .json(&body)
        .send() 
        .await?;

    // è§£æå›æ‡‰
    if res.status().is_success() {
        let response_json: Value = res.json().await?;
        
        // æŠ“å– choices[0].message.content
        if let Some(content) = response_json["choices"][0]["message"]["content"].as_str() {
            println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", content);
        } else {
            println!("âš ï¸ LLM å›æ‡‰æ ¼å¼ç„¡æ³•è§£æ (å¯èƒ½ç„¡å…§å®¹): {:?}", response_json);
        }
    } else {
        println!("âŒ LLM è«‹æ±‚å¤±æ•—: Status {}", res.status());
        // å˜—è©¦å°å‡ºéŒ¯èª¤è¨Šæ¯å¹«åŠ©é™¤éŒ¯
        println!("Response: {}", res.text().await?);
    }

    Ok(())
}

// --- å–®æª”è™•ç†æ ¸å¿ƒé‚è¼¯ (Core Logic) ---
async fn process_single_file(
    path: &Path, 
    db: &lancedb::Connection, 
    model: &mut TextEmbedding
) -> Result<(), Box<dyn Error>> {
    let filename = path.file_name().unwrap().to_str().unwrap();
    let path_str = path.to_str().unwrap();

    println!("--------------------------------------------------");
    println!("ğŸš€ é–‹å§‹è™•ç†: {}", filename);

    // [Check Idempotency] æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    // ç°¡å–®æŸ¥è©¢ DB æœ‰æ²’æœ‰é€™å€‹ filename
    if let Ok(table) = db.open_table(TABLE_NAME).execute().await {
        // ä½¿ç”¨ SQL style filter
        let filter = format!("source_file = '{}'", filename);
        let count = table.count_rows(Some(filter)).await?;
        if count > 0 {
            println!("â© æª”æ¡ˆå·²å­˜åœ¨ ({} ç­†ç´€éŒ„)ï¼Œè·³éè™•ç†: {}", count, filename);
            return Ok(());
        }
    }

    // Python è§£æ
    let doc = match run_python_parser(path_str) {
        Ok(d) => d,
        Err(e) => {
            eprintln!("âŒ è§£æå¤±æ•— [{}]: {}", filename, e);
            return Ok(()); // å›å‚³ Ok è®“è¿´åœˆç¹¼çºŒ
        }
    };

    // åˆ‡åˆ†
    let chunks = semantic_chunking(&doc, filename);
    if chunks.is_empty() {
        println!("âš ï¸  æª”æ¡ˆå…§å®¹ç‚ºç©ºï¼Œè·³éã€‚");
        return Ok(());
    }

    // Embedding (æ”¹åˆ†æ‰¹è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”)
    println!("ğŸ§  å‘é‡åŒ– {} å€‹ç‰‡æ®µ...", chunks.len());
    let batch_size = 30; 
    let mut embeddings = Vec::with_capacity(chunks.len());
    // ä½¿ç”¨ chunks() é€²è¡Œåˆ‡åˆ†
    for (_i, batch) in chunks.chunks(batch_size).enumerate() {
        // è½‰æˆ Vec<String> å‚³çµ¦ model
        let batch_vec = batch.to_vec();
        
        // åŸ·è¡Œå‘é‡åŒ–
        let batch_embeddings = model.embed(batch_vec, None)?;
        embeddings.extend(batch_embeddings);

        // æ¯ä¸€æ‰¹è™•ç†å®Œç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œè®“ CPU é™æº«
        thread::sleep(time::Duration::from_millis(50)); 
        
        // print!(".") ä¾†é¡¯ç¤ºé€²åº¦ï¼Œflush stdout ç¢ºä¿çœ‹å¾—åˆ°
        use std::io::{self, Write};
        print!(".");
        io::stdout().flush().unwrap();
    }
    println!("\nâœ… å‘é‡åŒ–å®Œæˆ");

    // æº–å‚™ Arrow Batch
    let total_rows = chunks.len();
    let dim = 768;

    let schema = Arc::new(Schema::new(vec![
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            dim as i32
        ), true),
        Field::new("product_name", DataType::Utf8, false),
        Field::new("source_file", DataType::Utf8, false), // æ–°å¢æ¬„ä½
    ]));

    let text_array = StringArray::from(chunks.clone());
    let product_array = StringArray::from(vec![doc.metadata.product_name.clone(); total_rows]);
    let source_array = StringArray::from(vec![filename.to_string(); total_rows]); 

    let mut list_builder = FixedSizeListBuilder::new(Float32Builder::with_capacity(total_rows * dim), dim as i32);
    for vector in &embeddings {
        list_builder.values().append_slice(vector);
        list_builder.append(true);
    }
    let vector_array = list_builder.finish();

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(text_array),
            Arc::new(vector_array),
            Arc::new(product_array),
            Arc::new(source_array),
        ],
    )?;

    // å¯«å…¥ DB (Append æ¨¡å¼)
    let table_names = db.table_names().execute().await?;
    if table_names.contains(&TABLE_NAME.to_string()) {
        let table = db.open_table(TABLE_NAME).execute().await?;
        // é€™è£¡éœ€è¦ç”¨ iterator åŒ…èµ·ä¾†
        let batches = RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        table.add(Box::new(batches)).execute().await?;
    } 
    else {
        // ç¬¬ä¸€æ¬¡å»ºç«‹
        let batches = arrow_array::RecordBatchIterator::new(vec![Ok(batch)], schema.clone());
        db.create_table(TABLE_NAME, Box::new(batches)).execute().await?;
    }

    println!("âœ… å®Œæˆ: {}", filename);
    Ok(())
}

/* for JSON and then */

// è®€å–å–®ä¸€ JSON æª”æ¡ˆ
fn load_policy_json(path: &Path) -> Result<models::PolicyData, Box<dyn Error>> {
    let content = fs::read_to_string(path)?;
    // ä½¿ç”¨ serde_json è§£æ
    let data: models::PolicyData = serde_json::from_str(&content)?;
    Ok(data)
}

fn chunk_policy_data(data: &models::PolicyData) -> Vec<String> {
    let mut chunks = Vec::new();
    let pname = &data.basic_info.product_name;
    let fname = &data.source_filename;
    
    // ğŸ”¥ğŸ”¥ğŸ”¥ã€æ™ºæ…§æ¨™ç±¤ç³»çµ±ã€‘ğŸ”¥ğŸ”¥ğŸ”¥
    // æˆ‘å€‘æ ¹æ“š JSON æ¬„ä½è‡ªå‹•æ¨å°å‡ºä½¿ç”¨è€…å¯èƒ½æœƒæœçš„å£èªé—œéµå­—
    let mut tags = Vec::new();

    // 1. æ ¸å¿ƒåˆ†é¡ (æŠ•è³‡ vs å‚³çµ±)
    if data.investment.is_investment_linked {
        tags.push("æŠ•è³‡å‹ä¿å–®".to_string());
        tags.push("åŸºé‡‘ä¿å–®".to_string()); // ä¿—ç¨±
        tags.push("è®Šé¡ä¿éšª".to_string());
        tags.push("ç†è²¡å‹ä¿éšª".to_string());
        tags.push("é«˜é¢¨éšªé«˜å ±é…¬".to_string()); // ç‰¹å¾µ
    } else {
        tags.push("å‚³çµ±å‹ä¿å–®".to_string());
        tags.push("å›ºå®šåˆ©ç‡".to_string());
        tags.push("ä¿è­‰çµ¦ä»˜".to_string());
    }

    // 2. åŠŸèƒ½éœ€æ±‚ (å­˜éŒ¢ vs ä¿éšœ vs é€€ä¼‘)
    let type_desc = &data.basic_info.product_type;
    let cov_death = &data.coverage.death_benefit;
    let cov_maturity = &data.coverage.maturity_benefit;

    // åˆ¤æ–·æ˜¯å¦ç‚ºã€Œå„²è“„/é€€ä¼‘ã€å°å‘
    // å¦‚æœæœ‰ã€Œæ»¿æœŸé‡‘ã€ã€ã€Œç”Ÿå­˜é‡‘ã€æˆ–é¡å‹æ˜¯ã€Œå¹´é‡‘ã€
    if type_desc.contains("å¹´é‡‘") || cov_maturity.len() > 5 { 
        tags.push("é€€ä¼‘è¦åŠƒ".to_string());
        tags.push("é¤Šè€é‡‘".to_string());
        tags.push("å„²è“„éšª".to_string()); // é›–ç„¶ç¾åœ¨æ³•è¦å°‘ç”¨æ­¤è©ï¼Œä½†æ°‘çœ¾æ„›æœ
        tags.push("å­˜éŒ¢".to_string());
        tags.push("ç¾é‡‘æµ".to_string());
    }

    // åˆ¤æ–·æ˜¯å¦ç‚ºã€Œç´”ä¿éšœ/å£½éšªã€å°å‘
    if type_desc.contains("å£½éšª") || cov_death.len() > 5 {
        tags.push("å£½éšªä¿éšœ".to_string());
        tags.push("èº«æ•…è³ å„Ÿ".to_string());
        tags.push("ç•™æ„›çµ¦å®¶äºº".to_string()); // è¡ŒéŠ·ç”¨èª
        tags.push("è³‡ç”¢å‚³æ‰¿".to_string());   // é«˜è³‡ç”¢æ—ç¾¤é—œéµå­—
    }

    // 3. å¹£åˆ¥ç‰¹æ€§ (ç¾å…ƒ/å¤–å¹£)
    let currencies = &data.basic_info.currency;
    if currencies.iter().any(|c| c.contains("USD") || c.contains("ç¾å…ƒ")) {
        tags.push("ç¾å…ƒä¿å–®".to_string());
        tags.push("ç¾é‡‘ä¿å–®".to_string());
        tags.push("å¼·å‹¢è²¨å¹£".to_string());
    }
    if currencies.iter().any(|c| c != "TWD" && c != "æ–°å°å¹£") {
        tags.push("å¤–å¹£ä¿å–®".to_string());
        tags.push("è³‡ç”¢é…ç½®".to_string());
    }

    // 4. ç¹³è²»æ–¹å¼ (èº‰ç¹³/æœŸç¹³)
    let payment = &data.basic_info.payment_period;
    if payment.contains("èº‰") || payment.contains("ä¸€æ¬¡") {
        tags.push("èº‰ç¹³".to_string());
        tags.push("ä¸€æ¬¡ç¹³æ¸…".to_string());
        tags.push("å–®ç­†æŠ•è³‡".to_string());
    } else {
        tags.push("æœŸç¹³".to_string());
        tags.push("åˆ†æœŸç¹³è²»".to_string());
    }

    // 5. ç‰¹æ®Šæ—ç¾¤ (é«˜é½¡/å°å­©)
    let target = &data.rag_data.target_audience;
    if target.contains("65æ­²") || target.contains("é«˜é½¡") {
        tags.push("éŠ€é«®æ—ä¿å–®".to_string());
        tags.push("é«˜é½¡æŠ•ä¿".to_string());
    }
    if target.contains("å°å­©") || target.contains("å­å¥³") {
        tags.push("å…’ç«¥ä¿å–®".to_string());
        tags.push("æ•™è‚²åŸºé‡‘".to_string());
    }

    // å°‡åŸæœ¬çš„é—œéµå­—ä¹ŸåŠ é€²ä¾† (å»é‡)
    for kw in &data.rag_data.keywords {
        if !tags.contains(kw) {
            tags.push(kw.clone());
        }
    }

    // ç”Ÿæˆæ¨™ç±¤å­—ä¸²ï¼Œä¾‹å¦‚: "[TAGS: æŠ•è³‡å‹, åŸºé‡‘, ç¾å…ƒä¿å–®, é€€ä¼‘]"
    let tags_str = format!("[é—œéµå­—: {}]", tags.join(", "));
    // ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

    // ä¿®æ”¹ Headerï¼ŒæŠŠé€™äº›å¼·å¤§çš„æ¨™ç±¤åŸ‹é€²æ¯ä¸€å€‹å‘é‡ç‰‡æ®µ
    let header = format!("å•†å“: {} | ä¾†æº: {} | {}", pname, fname, tags_str);

    // --- ä»¥ä¸‹ Chunk ç”Ÿæˆé‚è¼¯ä¿æŒä¸è®Šï¼Œä½†å› ç‚º Header è®Šå¼·äº†ï¼Œæ‰€æœ‰ Chunk éƒ½è®Šå¼·äº† ---

    // Chunk 1: åŸºæœ¬è³‡è¨Š
    let chunk_basic = format!(
        "{} | [åŸºæœ¬è³‡è¨Š] æ–‡è™Ÿ: {} | é¡å‹: {} | ç¹³è²»: {} | å¹£åˆ¥: {:?} | æŠ•ä¿å¹´é½¡: {} | ä¿è²»é–€æª»: {}",
        header,
        data.basic_info.product_code,
        data.basic_info.product_type,
        data.basic_info.payment_period,
        data.basic_info.currency,
        data.conditions.age_range,
        data.conditions.premium_limit
    );
    chunks.push(chunk_basic);

    // Chunk 2: ä¿éšœå…§å®¹
    let chunk_cov = format!(
        "{} | [ä¿éšœå…§å®¹] èº«æ•…/å–ªè‘¬çµ¦ä»˜: {} | æ»¿æœŸ/ç¥å£½çµ¦ä»˜: {} | å…¶ä»–æ¬Šç›Š: {:?}",
        header,
        data.coverage.death_benefit,
        data.coverage.maturity_benefit,
        data.coverage.other_benefits
    );
    chunks.push(chunk_cov);

    // Chunk 3: æŠ•è³‡ç‰¹è‰²
    if data.investment.is_investment_linked {
        let chunk_inv = format!(
            "{} | [æŠ•è³‡ç‰¹è‰²] æ­¤ç‚ºæŠ•è³‡å‹ä¿å–®(åŸºé‡‘/å…¨å§”)ã€‚ç‰¹è‰²: {:?} | é¢¨éšª: {:?}",
            header,
            data.investment.features,
            data.investment.risks
        );
        chunks.push(chunk_inv);
    }

    // Chunk 4: è²»ç”¨
    let chunk_fee = format!("{} | [è²»ç”¨èªªæ˜] {}", header, data.conditions.fees_and_discounts);
    chunks.push(chunk_fee);

    // Chunk 5: å®¢ç¾¤
    let chunk_meta = format!("{} | [é©ç”¨å®¢ç¾¤] {} | é¡å¤–æ¨™ç±¤: {:?}", header, data.rag_data.target_audience, tags);
    chunks.push(chunk_meta);

    // Chunk 6: FAQ
    for faq in &data.rag_data.faq {
        let chunk_faq = format!("{} | [å¸¸è¦‹å•é¡Œ] Q: {} | A: {}", header, faq.q, faq.a);
        chunks.push(chunk_faq);
    }

    chunks
}

// --- 2. è™•ç†å–®ä¸€æª”æ¡ˆæµç¨‹ (Embedding + DB Insert) ---
async fn process_and_index_json(
    path: &Path,
    table: &lancedb::Table,
    model: &mut TextEmbedding
) -> Result<(), Box<dyn Error>> {
    let filename = path.file_name().unwrap().to_str().unwrap();
    let content = fs::read_to_string(path)?;
    let current_hash = calculate_hash(&content);

    // 1. è®€å– JSON
    let policy = match load_policy_json(path) {
        Ok(p) => p,
        Err(e) => {
            eprintln!("âŒ JSON è§£æå¤±æ•— {}: {}", filename, e);
            return Ok(());
        }
    };
    // ğŸ”¥ğŸ”¥ğŸ”¥ã€DIFF æ ¸å¿ƒé‚è¼¯ã€‘ğŸ”¥ğŸ”¥ğŸ”¥
    // æŸ¥è©¢ DB ä¸­æ˜¯å¦å·²æœ‰æ­¤æª”æ¡ˆï¼Œä¸¦å–å‡ºå®ƒçš„ file_hash
    let where_clause = format!("source_file = '{}'", policy.source_filename);
    
    let results = table
        .query()
        .only_if(where_clause)
        .limit(1) // åªè¦æŸ¥ä¸€ç­†å°±çŸ¥é“æœ‰æ²’æœ‰
        .execute()
        .await?;

    let batches: Vec<RecordBatch> = results.try_collect().await?;
    
    if let Some(batch) = batches.first() {
        if batch.num_rows() > 0 {
            // DB è£¡æœ‰é€™å€‹æª”ï¼Œæª¢æŸ¥ Hash æ˜¯å¦ä¸€æ¨£
            if let Some(hash_col) = batch.column_by_name("file_hash") {
                 if let Some(str_array) = hash_col.as_any().downcast_ref::<StringArray>() {
                     let db_hash = str_array.value(0); // å–ç¬¬ä¸€åˆ—çš„ Hash
                     
                     if db_hash == current_hash {
                         println!("â© [è·³é] æª”æ¡ˆæœªè®Šæ›´: {}", filename);
                         return Ok(()); // Hash ä¸€æ¨£ï¼Œå®Œå…¨ä¸åšäº‹
                     } else {
                         println!("ğŸ”„ [æ›´æ–°] æª”æ¡ˆå…§å®¹å·²è®Šæ›´ (Hashä¸åŒ)ï¼Œé‡æ–°ç´¢å¼•: {}", filename);
                         // Hash ä¸åŒï¼Œç¨‹å¼æœƒç¹¼çºŒå¾€ä¸‹èµ°ï¼ŒåŸ·è¡Œåˆªé™¤èˆŠè³‡æ–™+å¯«å…¥æ–°è³‡æ–™
                     }
                 }
            }
        }
    }
    // ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
    // 2. åˆ‡åˆ†
    let chunks = chunk_policy_data(&policy);
    if chunks.is_empty() { return Ok(()); }

    println!("ğŸ”„ æ­£åœ¨ç´¢å¼•: {} (ç”¢ç”Ÿ {} å€‹å‘é‡ç‰‡æ®µ)", filename, chunks.len());

    // 3. å‘é‡åŒ– (Embedding)
    let embeddings = model.embed(chunks.clone(), None)?;

    // 4. æº–å‚™å¯«å…¥ LanceDB çš„è³‡æ–™
    let total_chunks = chunks.len();
    let embedding_dim = 768; // BGE-Base çš„ç¶­åº¦

    // å»ºæ§‹ Arrow Arrays
    let source_array = StringArray::from(vec![policy.source_filename.clone(); total_chunks]);
    let hash_array = StringArray::from(vec![current_hash; total_chunks]);
    let text_array = StringArray::from(chunks);
    
    // å»ºæ§‹å‘é‡ Array (Flattened list)
    let mut vector_builder = FixedSizeListBuilder::new(
        arrow_array::builder::Float32Builder::new(),
        embedding_dim as i32,
    );
    
    for vec in embeddings {
        for val in vec {
            vector_builder.values().append_value(val);
        }
        vector_builder.append(true);
    }
    let vector_array = vector_builder.finish();

    // å»ºç«‹ RecordBatch
    let schema = Arc::new(Schema::new(vec![
        Field::new("source_file", DataType::Utf8, false),
        Field::new("file_hash", DataType::Utf8, false),
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            embedding_dim as i32
        ), false),
    ]));

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(source_array),
            Arc::new(hash_array),
            Arc::new(text_array),
            Arc::new(vector_array),
        ],
    )?;

    // 5. å¯«å…¥ DB (å…ˆåˆªé™¤èˆŠçš„å†å¯«å…¥ï¼Œç¢ºä¿ä¸é‡è¤‡)
    // æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ç”¨ source_filename ä¾†åˆªé™¤ï¼Œé€™å°æ‡‰åˆ°åŸå§‹ PDF/DOCX æª”å
    let delete_filter = format!("source_file = '{}'", policy.source_filename);
    table.delete(&delete_filter).await?;

    let batches = RecordBatchIterator::new(vec![Ok(batch)], schema.clone());

    
    table.add(Box::new(batches)).execute().await?;

    Ok(())
}

// --- 3. å•ç­”é‚è¼¯ ---
async fn handle_user_query(
    db: &lancedb::Connection, 
    model: &mut TextEmbedding, 
    user_query: &str,
    synonyms: &HashMap<String, String>,
    summaries: &HashMap<String, ProductSummary>
) -> Result<(), Box<dyn Error>> {

    // --- è®€å–ç’°å¢ƒè®Šæ•¸ (è¨­å®šé è¨­å€¼ä»¥é˜²æ²’è¨­) ---
    let recall_limit = env::var("RAG_RECALL_LIMIT")
        .unwrap_or("20".to_string())
        .parse::<usize>()
        .unwrap_or(20);

    let rerank_limit = env::var("RAG_RERANK_LIMIT")
        .unwrap_or("3".to_string())
        .parse::<usize>()
        .unwrap_or(3);

    let rerank_api = env::var("RERANK_API_URL")
        .unwrap_or("http://localhost:8000/rerank".to_string());
    // -------------------------------------

    // 0. å­—å…¸æ“´å……
    let mut final_query = user_query.to_string();
    for (slang, term) in synonyms {
        if user_query.contains(slang) {
            println!("ğŸ’¡ [å­—å…¸å‘½ä¸­] '{}' -> åŠ ä¸Š '{}'", slang, term);
            final_query.push_str(" ");
            final_query.push_str(term);
        }
    }

    // 1. å‘é‡åŒ–å•é¡Œ
    // let query_embedding = model.embed(vec![user_query.to_string()], None)?;
    // let query_vector = query_embedding[0].clone();
    let query_vec = model.embed(vec![final_query.clone()], None)?[0].clone();

    // 2. æœå°‹ DB
    let table = db.open_table(TABLE_NAME).execute().await?;
    let results = table
        .query()
        .nearest_to(query_vec)?
        .limit(recall_limit) // å–å‰ 3 å€‹æœ€ç›¸é—œçš„ç‰‡æ®µ
        .execute()
        .await?;
    
    let batches: Vec<RecordBatch> = results.try_collect().await?;

     // 3. æª¢æŸ¥çµæœ (ç°¡æ˜“ä¿¡å¿ƒæª¢æŸ¥: æœ‰æ²’æœ‰çµæœ)
    let has_results = !batches.is_empty() && batches[0].num_rows() > 0;

    let mut used_batches = batches;

    // 4. AI è£œæ•‘ (å¦‚æœæ²’çµæœ)
    if !has_results {
        println!("âš ï¸  åˆæ­¥æœå°‹ç„¡çµæœï¼Œå˜—è©¦ AI æ·±åº¦æ“´å……...");
        if let Some(ai_kw) = expand_query_with_ai(user_query).await {
            let ai_vec = model.embed(vec![ai_kw], None)?[0].clone();
            let ai_results = table.query().nearest_to(ai_vec)?.limit(3).execute().await?;
            used_batches = ai_results.try_collect().await?;
        }
    }

    // ğŸ”¥ğŸ”¥ğŸ”¥ æ’å…¥ Re-ranking æ­¥é©Ÿ ğŸ”¥ğŸ”¥ğŸ”¥
    let top_results = rerank_documents(user_query, used_batches, summaries, rerank_limit, &rerank_api).await?;
    
    if top_results.is_empty() {
         println!("âŒ ç¶“é Re-ranking å¾Œç„¡åˆé©è³‡æ–™ã€‚");
         return Ok(());
    }

    // 5. çµ„è£ Context (åŒ…å«å•†å“æ‘˜è¦)
    let mut hit_files = HashSet::new();
    let mut snippets_text = String::new();

    println!("\nğŸ” [RAG æª¢ç´¢çµæœ]");
   
    for (src, txt, score) in &top_results {
        hit_files.insert(src.clone());
        // æˆ‘å€‘å¯ä»¥åœ¨ context è£¡ç¨å¾®æ¨™è¨»ä¸€ä¸‹é€™æ˜¯ç²¾é¸å‡ºä¾†çš„
        snippets_text.push_str(&format!("ğŸ“„ [ç²¾é¸ç‰‡æ®µ] (é—œè¯åº¦:{:.1}) ä¾†æº: {}\nå…§å®¹: {}\n\n", score, src, txt));
    }

    /* if context_buffer.is_empty() {
        println!("âš ï¸  æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚");
        return Ok(());
    } */
    if hit_files.is_empty() {
        println!("âš ï¸  æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚");
        // é€™è£¡å¯ä»¥è€ƒæ…®å‘¼å« AI Expansion
        return Ok(());
    }

    // 6. æ³¨å…¥æ‘˜è¦ (Summary Injection)
    let mut final_context = String::new();
    final_context.push_str("=== ç›¸é—œå•†å“åŸºæœ¬ä»‹ç´¹ ===\n");
    for filename in &hit_files {
        if let Some(summary) = summaries.get(filename) {
            final_context.push_str(&format!("ğŸ“„ ä¾†æº: {}\n{}\n", filename, summary.intro));
        }
    }
    final_context.push_str("========================\n\n");
    final_context.push_str("=== è©³ç´°æª¢ç´¢ç‰‡æ®µ ===\n");
    final_context.push_str(&snippets_text);

    // 7. æœ€å¾Œç”Ÿæˆ
    //ask_llm(&final_context, user_query).await?;
    
    println!("\nğŸ“š [ç³»çµ±åƒè€ƒä¾†æºæ–‡ä»¶]");
    let mut sorted_files: Vec<_> = hit_files.into_iter().collect();
    sorted_files.sort(); // æ’å€‹åºæ¯”è¼ƒå¥½çœ‹
    for (idx, filename) in sorted_files.iter().enumerate() {
        // å¦‚æœæœ‰æ‘˜è¦ï¼Œé †ä¾¿å°å‡ºå•†å“åç¨±ï¼Œæ›´æ¸…æ¥š
        if let Some(summary) = summaries.get(filename) {
            println!(" {}. {} ({})", idx + 1, summary.name, filename);
        } else {
            println!(" {}. {}", idx + 1, filename);
        }
    }
    println!("==================================");
    // println!("ğŸ¤– (LLM æœƒæ ¹æ“šä¸Šè¿° Context å›ç­”æ‚¨çš„å•é¡Œ: '{}')", user_query);
    // println!("ğŸ“š åƒè€ƒæ–‡ä»¶: {:?}", sources);

    Ok(())
}

// å›å‚³ (æ‘˜è¦Map, åŒç¾©è©Map)
fn load_data_from_json_dir() -> (HashMap<String, ProductSummary>, HashMap<String, String>) {
    let mut summaries = HashMap::new();
    let mut synonyms = HashMap::new();
    
    println!("ğŸš€ Rust æ­£åœ¨æƒæ JSON è³‡æ–™å¤¾å»ºç«‹å¿«å–...");
    
    let walker = WalkDir::new(PROCESSED_JSON_DIR).into_iter();
    
    for entry in walker.filter_map(|e| e.ok()) {
        let path = entry.path();
        if path.extension().map_or(false, |e| e == "json") {
            if let Ok(content) = fs::read_to_string(path) {
                // å˜—è©¦è§£æ JSON
                if let Ok(data) = serde_json::from_str::<models::PolicyData>(&content) {
                    
                    // --- 1. è™•ç†æ‘˜è¦ (åŸæœ‰é‚è¼¯) ---
                    let intro = format!(
                        "ã€å•†å“ç¸½è¦½ã€‘\nåç¨±: {}\né¡å‹: {}\nç‰¹è‰²: {:?}\né©åˆå°è±¡: {}\n",
                        data.basic_info.product_name,
                        data.basic_info.product_type,
                        data.investment.features,
                        data.rag_data.target_audience
                    );

                    summaries.insert(data.source_filename.clone(), ProductSummary {
                        name: data.basic_info.product_name,
                        intro,
                    });

                    // --- 2. è™•ç†åŒç¾©è© (æ–°å¢é‚è¼¯) ---
                    // å‡è¨­ models::RagData è£¡é¢æœ‰ synonym_mapping æ¬„ä½
                    // æ³¨æ„ï¼šæ‚¨éœ€è¦åœ¨ models.rs è£¡å°æ‡‰åŠ ä¸Šé€™å€‹æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰çš„è©±
                    if let Some(mapping) = &data.rag_data.synonym_mapping {
                        for entry in mapping {
                            // è™•ç†é€—è™Ÿåˆ†éš” (ä¾‹å¦‚: "æ­»æ‰, èµ°äº†")
                            let slangs: Vec<&str> = entry.slang.split(&['ã€', ','][..]).collect();
                            for s in slangs {
                                let clean_s = s.trim().to_string();
                                if !clean_s.is_empty() {
                                    // å»ºç«‹åå‘ç´¢å¼•: å£èª -> å°ˆæ¥­è¡“èª
                                    synonyms.insert(clean_s, entry.formal.clone());
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    println!("ğŸ“š è³‡æ–™è¼‰å…¥å®Œæˆï¼");
    println!("   - å•†å“æ‘˜è¦: {} ç­†", summaries.len());
    println!("   - åŒç¾©è©åº«: {} ç­†", synonyms.len());
    
    (summaries, synonyms)
}

fn load_product_summaries() -> HashMap<String, ProductSummary> {
    let mut summaries = HashMap::new();
    let walker = WalkDir::new(PROCESSED_JSON_DIR).into_iter();
    
    for entry in walker.filter_map(|e| e.ok()) {
        let path = entry.path();
        if path.extension().map_or(false, |e| e == "json") {
            if let Ok(content) = fs::read_to_string(path) {
                if let Ok(data) = serde_json::from_str::<models::PolicyData>(&content) {
                    
                    // ğŸ”¥ çµ„åˆå‡ºä¸€å€‹æœ€å¼·çš„ã€Œå•†å“å±¥æ­·ã€
                    let intro = format!(
                        "ã€å•†å“ç¸½è¦½ã€‘\nåç¨±: {}\né¡å‹: {}\nç‰¹è‰²: {:?}\né©åˆå°è±¡: {}\n",
                        data.basic_info.product_name,
                        data.basic_info.product_type,
                        data.investment.features, // å¦‚æœæ˜¯å‚³çµ±å‹é€™è£¡å¯èƒ½æ˜¯ç©ºï¼Œæ²’é—œä¿‚
                        data.rag_data.target_audience
                    );

                    summaries.insert(data.source_filename.clone(), ProductSummary {
                        name: data.basic_info.product_name,
                        intro,
                    });
                }
            }
        }
    }
    println!("ğŸ“š å·²å¿«å– {} ç­†å•†å“æ‘˜è¦", summaries.len());
    summaries
}

fn load_synonyms() -> HashMap<String, String> {
    if let Ok(content) = fs::read_to_string(SYNONYMS_PATH) {
        // å‡è¨­ JSON æ ¼å¼æ˜¯ {"mapping": {"å£èª": "è¡“èª"}}ï¼Œé€™è£¡ç°¡åŒ–è™•ç†ç›´æ¥è®€ Map
        // å¦‚æœæ‚¨çš„ Python ç”¢å‡ºæ˜¯ç›´æ¥çš„ Dictï¼Œé€™æ¨£å¯«æ˜¯å°çš„
        if let Ok(map) = serde_json::from_str::<HashMap<String, String>>(&content) {
            println!("ğŸ“š è¼‰å…¥é›¢ç·šåŒç¾©è©å­—å…¸: {} ç­†", map.len());
            return map;
        } 
        // å¦‚æœ Python ç”¢å‡ºåŒ…å« "mapping" keyï¼Œè«‹æ”¹ç”¨ Value è§£æ
    }
    println!("âš ï¸ ç„¡æ³•è¼‰å…¥å­—å…¸ ({}). å°‡åªä½¿ç”¨åŸå­—ä¸²æœå°‹ã€‚", SYNONYMS_PATH);
    HashMap::new()
}

async fn expand_query_with_ai(query: &str) -> Option<String> {
    println!("ğŸ¤– [AI ä»‹å…¥] æ­£åœ¨è«‹æ±‚åœ°ç«¯ LLM (Gemma 27B) åˆ†ææ„åœ–: '{}'...", query);
    
    // 1. è®€å–ç’°å¢ƒè®Šæ•¸ (èˆ‡ ask_llm ä½¿ç”¨ç›¸åŒçš„è¨­å®š)
    let vllm_endpoint = std::env::var("VLLM_ENDPOINT")
        .unwrap_or("http://localhost:11434".to_string());
    let model_name = std::env::var("MODEL_NAME")
        .unwrap_or("gemma2:27b".to_string());
    let token = std::env::var("BEARER_TOKEN").unwrap_or_default();

    // 2. è™•ç† URL (ç¢ºä¿æŒ‡å‘ chat/completions)
    let base_url = vllm_endpoint.trim_end_matches('/'); 
    let api_url = if base_url.contains("/v1") {
        format!("{}/chat/completions", base_url)
    } else {
        format!("{}/v1/chat/completions", base_url)
    };

    // 3. è¨­è¨ˆ Prompt (é€™æ˜¯é—œéµï¼)
    // æˆ‘å€‘è¦åš´æ ¼é™åˆ¶ Gemma ä¸è¦ "èŠå¤©"ï¼Œåªæº– "å·¥ä½œ"
    let system_prompt = "ä½ æ˜¯ä¿éšªé—œéµå­—å°ˆå®¶ã€‚è«‹å°‡ä½¿ç”¨è€…çš„æœå°‹è½‰æ›ç‚º 3-5 å€‹å°ç£ä¿éšªå°ˆæ¥­è¡“èªï¼Œä»¥åˆ©è³‡æ–™åº«æª¢ç´¢ã€‚\
                         \nè¦å‰‡ï¼š\
                         \n1. çµ•å°ä¸è¦è¼¸å‡ºä»»ä½•è§£é‡‹ã€é–‹å ´ç™½æˆ–çµå°¾ã€‚\
                         \n2. åªè¼¸å‡ºé—œéµå­—ï¼Œç”¨ç©ºç™½åˆ†éš”ã€‚\
                         \n3. ä¾‹å¦‚ï¼šè¼¸å…¥ã€æ­»æ‰è³ éŒ¢ã€ï¼Œè¼¸å‡ºã€èº«æ•…çµ¦ä»˜ å£½éšªä¿éšœ å–ªè‘¬è²»ç”¨ã€ã€‚";
    
    let user_prompt = format!("ä½¿ç”¨è€…æœå°‹: '{}'", query);

    let client = reqwest::Client::builder()
        .no_proxy()
        .build()
        .ok()?;

    let request_body = json!({
        "model": model_name,
        "messages": [
            { "role": "system", "content": system_prompt },
            { "role": "user", "content": user_prompt }
        ],
        "temperature": 0.1, // ä½æº«ï¼Œè®“å®ƒå°ˆæ³¨ä¸ç™¼æ•£
        "stream": false
    });

    let mut request_builder = client.post(&api_url)
        .header("Content-Type", "application/json");

    // Token è™•ç†
    let token_check = token.trim().to_lowercase();
    if !["", "none", "null"].contains(&token_check.as_str()) {
        request_builder = request_builder.header("Authorization", format!("Bearer {}", token));
    }

    // 4. ç™¼é€èˆ‡è§£æ
    match request_builder.json(&request_body).send().await {
        Ok(resp) => {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(text) = json["choices"][0]["message"]["content"].as_str() {
                    let clean_text = text.trim().replace("\n", " ");
                    // æœ‰æ™‚å€™ LLM æœƒå¿ä¸ä½åŠ  "é—œéµå­—ï¼š"ï¼Œæˆ‘å€‘æŠŠå®ƒæ¿¾æ‰
                    let clean_text = clean_text.replace("é—œéµå­—ï¼š", "").replace("Keywords:", "");
                    
                    println!("âœ¨ AI å»ºè­°é—œéµå­—: {}", clean_text);
                    return Some(clean_text);
                }
            }
        }
        Err(e) => eprintln!("âŒ åœ°ç«¯ API å‘¼å«å¤±æ•—: {}", e),
    }
    None
}

// --- LLM APIï¼šæ“´å……é—œéµå­— (Query Expansion) é€€ä¼‘å¾Œå†ç”¨ ---
async fn expand_query_with_ai_future(query: &str) -> Option<String> {
    let api_key = std::env::var("GOOGLE_API_KEY").ok()?;
    println!("ğŸ¤– [AI ä»‹å…¥] æ­£åœ¨è«‹æ±‚ Gemini åˆ†ææ„åœ–: '{}'...", query);
    
    let client = reqwest::Client::new();
    let prompt = format!("ä½¿ç”¨è€…æœå°‹: '{}'ã€‚è«‹è½‰æ›ç‚º3å€‹å°ç£ä¿éšªå°ˆæ¥­é—œéµå­—(å¦‚:è®Šé¡è¬èƒ½å£½éšª, æœˆæ’¥å›)ï¼Œç”¨ç©ºç™½åˆ†éš”ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚", query);

    let request_body = json!({
        "contents": [{ "parts": [{ "text": prompt }] }]
    });

    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={}", api_key);

    match client.post(&url).json(&request_body).send().await {
        Ok(resp) => {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(text) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                    let clean = text.trim().replace("\n", " ");
                    println!("âœ¨ AI å»ºè­°é—œéµå­—: {}", clean);
                    return Some(clean);
                }
            }
        }
        Err(_) => {}
    }
    None
}

async fn rerank_documents(
    query: &str,
    batches: Vec<RecordBatch>,
    summaries: &HashMap<String, ProductSummary>,
    top_k: usize,
    api_url: &str
) -> Result<Vec<(String, String, f32)>, Box<dyn Error>> {

    let max_chunks_per_doc = env::var("MAX_CHUNKS_PER_DOC")
        .unwrap_or("3".to_string())
        .parse::<usize>()
        .unwrap_or(3);
    
    // 1. å…ˆæŠŠæ‰€æœ‰ LanceDB çš„çµæœè§£é–‹æˆç´”æ–‡å­—åˆ—è¡¨
    let mut raw_docs: Vec<(String, String)> = Vec::new(); // (source, text)
    let mut doc_texts_for_api: Vec<String> = Vec::new();

    for batch in &batches {
        let text_col = batch.column_by_name("text").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        let src_col = batch.column_by_name("source_file").unwrap().as_any().downcast_ref::<StringArray>().unwrap();
        
        for i in 0..batch.num_rows() {
            let src = src_col.value(i).to_string();
            let txt = text_col.value(i).to_string();
            
            // ç‚ºäº†è®“ Re-ranker åˆ¤æ–·æº–ç¢ºï¼Œæˆ‘å€‘æŠŠã€Œæ‘˜è¦ã€ä¹ŸåŠ é€²å»çµ¦å®ƒè®€
            // é€™æ¨£å®ƒæ‰çŸ¥é“ "å„ªåˆ©ç²¾é¸" æ˜¯æŠ•è³‡å‹ä¿å–®
            let content_for_judge = if let Some(sum) = summaries.get(&src) {
                format!("{}\næ–‡ä»¶å…§å®¹: {}", sum.intro, txt)
            } else {
                txt.clone()
            };

            raw_docs.push((src, txt));
            doc_texts_for_api.push(content_for_judge);
        }
    }

    if raw_docs.is_empty() {
        return Ok(Vec::new());
    }

    // 2. å‘¼å« Python Re-ranker API
    let client = reqwest::Client::new();
    let request_body = RerankRequest {
        query: query.to_string(),
        documents: doc_texts_for_api,
    };

    println!("âš–ï¸ æ­£åœ¨é€²è¡Œ Re-ranking ({} ç­†å€™é¸, å– Top {})...", raw_docs.len(), top_k);

    // å‡è¨­ Python Server è·‘åœ¨ localhost:8009
    // å¦‚æœæ‚¨ç”¨ Docker æˆ–å…¶ä»–é…ç½®ï¼Œè«‹æ”¹ URL
    let resp = client.post(api_url)
        .json(&request_body)
        .send()
        .await?;

    let rerank_res: RerankResponse = resp.json().await?;

    // 3. æ ¹æ“šå›å‚³çš„ indices é‡æ–°çµ„è£çµæœ
    let mut ranked_results = Vec::new();
    let mut file_counts: HashMap<String, usize> = HashMap::new();
    
    for (i, &original_idx) in rerank_res.indices.iter().enumerate() {
        if ranked_results.len() >= top_k { break; }
        
        let score = rerank_res.scores[i];
        
        // ğŸ’¡ é€²éšæŠ€å·§ï¼šå¯ä»¥åœ¨é€™è£¡è¨­ä¸€å€‹ã€Œé–€æª»å€¼ã€
        // BGE-Re-ranker çš„åˆ†æ•¸é€šå¸¸åœ¨ -10 ~ +10 ä¹‹é–“ (Logits)
        // è² åˆ†é€šå¸¸ä»£è¡¨ä¸ç›¸é—œ
        if score < -5.0 { 
            continue; 
        }

        let (src, txt) = &raw_docs[original_idx];
        // æª¢æŸ¥é€™ä»½æª”æ¡ˆæ˜¯å¦å·²ç¶“é¡æ»¿
        let count = file_counts.entry(src.clone()).or_insert(0);
        
        if *count < max_chunks_per_doc {
            println!("   â­ [Top {}] åˆ†æ•¸: {:.2} | ä¾†æº: {}", i+1, score, src);
            ranked_results.push((src.clone(), txt.clone(), score));
            *count += 1;
        }
        else {
            println!("   â­ï¸ [è·³é] æª”æ¡ˆé¡æ»¿ ({}/{}): {:.2} | ä¾†æº: {}", *count, max_chunks_per_doc, score, src);
        }
    }

    Ok(ranked_results)
}

// --- LLM APIï¼šæœ€çµ‚å›ç­” (RAG Generation) é€™éƒ¨åˆ†é€€ä¼‘å¾Œç”¨ ---
async fn ask_llm_with_context(context: &str, question: &str) -> Result<(), Box<dyn Error>> {
    let api_key = std::env::var("GOOGLE_API_KEY").expect("GOOGLE_API_KEY not found");
    
    let client = reqwest::Client::new();
    let system_prompt = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¿éšªé¡§å•ã€‚è«‹æ ¹æ“šæä¾›çš„ã€å•†å“ä»‹ç´¹ã€‘èˆ‡ã€è©³ç´°ç‰‡æ®µã€‘å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚è‹¥è³‡æ–™ä¸è¶³è«‹èª å¯¦å‘ŠçŸ¥ã€‚";
    let full_prompt = format!("{}\n\nåƒè€ƒè³‡æ–™:\n{}\n\nä½¿ç”¨è€…å•é¡Œ: {}", system_prompt, context, question);

    let request_body = json!({
        "contents": [{ "parts": [{ "text": full_prompt }] }]
    });

    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={}", api_key);

    println!("ğŸ¤– æ­£åœ¨è©¢å• LLM (ç”Ÿæˆå›ç­”ä¸­)...");
    match client.post(&url).json(&request_body).send().await {
        Ok(resp) => {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(text) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                    println!("\nğŸ’¬ LLM å›ç­”ï¼š\n==================================\n{}\n==================================", text);
                } else {
                    println!("âŒ LLM å›å‚³æ ¼å¼éŒ¯èª¤æˆ–ç„¡å…§å®¹");
                }
            } else {
                println!("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰");
            }
        }
        Err(e) => println!("âŒ API å‘¼å«å¤±æ•—: {}", e),
    }
    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    dotenv().ok();

    // 1. åˆå§‹åŒ– DB
    let db = connect(DB_URI).execute().await?;
    println!("ğŸ’¾ é€£ç·šè‡³è³‡æ–™åº«: {}", DB_URI);

    // å»ºç«‹ Table (å¦‚æœä¸å­˜åœ¨)
    // æ³¨æ„: é€™è£¡å®šç¾© Schema
    let embedding_dim = 768;
    let schema = Arc::new(Schema::new(vec![
        Field::new("source_file", DataType::Utf8, false),
        Field::new("file_hash", DataType::Utf8, false), // â˜… æ–°å¢é€™ä¸€æ¬„
        Field::new("text", DataType::Utf8, false),
        Field::new("vector", DataType::FixedSizeList(
            Arc::new(Field::new("item", DataType::Float32, true)),
            embedding_dim
        ), false),
    ]));

    let table_names = db.table_names().execute().await?;
    let table = if table_names.contains(&TABLE_NAME.to_string()) {
        println!("ğŸ“‚ è³‡æ–™è¡¨ '{}' å·²å­˜åœ¨ï¼Œé–‹å•Ÿä¸­...", TABLE_NAME);
        db.open_table(TABLE_NAME).execute().await?
    } 
    else {
        println!("âœ¨ è³‡æ–™è¡¨ '{}' ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸­...", TABLE_NAME);
        // å»ºç«‹ä¸€å€‹ç©ºçš„è¿­ä»£å™¨ä¾†åˆå§‹åŒ–è¡¨çµæ§‹
        let batches: Vec<Result<RecordBatch, arrow_schema::ArrowError>> = vec![]; 
        db.create_table(TABLE_NAME, RecordBatchIterator::new(batches, schema.clone()))
            .execute()
            .await?
    };

    // 2. åˆå§‹åŒ– Embedding æ¨¡å‹
    println!("ğŸ§  è¼‰å…¥ Embedding æ¨¡å‹ (BGE-Base)...");
    let mut model = TextEmbedding::try_new(InitOptions::new(EmbeddingModel::BGEBaseENV15))?;
    // let synonyms = load_synonyms();           // <--- é€™è£¡ç”¢ç”Ÿ synonyms
    // let summaries = load_product_summaries(); // <--- é€™è£¡ç”¢ç”Ÿ summaries
    let (summaries, synonyms) = load_data_from_json_dir();

    // 3. æƒæä¸¦ç´¢å¼• JSON
    println!("\nğŸš€ é–‹å§‹ç´¢å¼• JSON è³‡æ–™å¤¾: {}", PROCESSED_JSON_DIR);
    if Path::new(PROCESSED_JSON_DIR).exists() {
        let walker = WalkDir::new(PROCESSED_JSON_DIR).into_iter();
        for entry in walker.filter_map(|e| e.ok()) {
            let path = entry.path();
            if path.extension().map_or(false, |e| e == "json") {
                // å‘¼å«ç´¢å¼•å‡½å¼
                if let Err(e) = process_and_index_json(path, &table, &mut model).await {
                    eprintln!("âŒ ç´¢å¼•éŒ¯èª¤: {:?}", e);
                }
            }
        }
    } else {
        println!("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° {} è³‡æ–™å¤¾ï¼Œè«‹ç¢ºèª Python è…³æœ¬æ˜¯å¦åŸ·è¡ŒæˆåŠŸã€‚", PROCESSED_JSON_DIR);
    }
    
    println!("\nâœ… æ‰€æœ‰è³‡æ–™ç´¢å¼•å®Œæˆï¼");

    // 4. äº’å‹•æ¨¡å¼
    println!("\nğŸ¤– ä¿éšª AI é¡§å• (RAG CLI) å·²å°±ç·’");
    println!("ğŸ’¡ è¼¸å…¥å•é¡Œ (ä¾‹å¦‚: 'å®‰è¯æ–°å‰æ˜Ÿæœ‰ä»€éº¼è²»ç”¨?' æˆ– 'exit' é›¢é–‹)");
    
    loop {
        print!("\nUser > ");
        io::stdout().flush()?;
        let mut input = String::new();
        if io::stdin().read_line(&mut input).is_ok() {
            let q = input.trim();
            if q.eq_ignore_ascii_case("exit") { break; }
            if q.is_empty() { continue; }

            handle_user_query(&db, &mut model, q, &synonyms, &summaries).await?;
        }
    }

    Ok(())
}

